{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Injectivity-o-matic\n",
    "# - code workbook to generate injectivity indices, frac pressures etc. for all injectors\n",
    "# - flags all periods of injection life as being bean-up, steady-state, shut-in etc. \n",
    "# - runs some cool algorithms to scrub P(Q) data and do (piecewise) linear regressions of these periods\n",
    "#\n",
    "# - this code workbook generates all of the underlying data sets; a separate code workbook / \n",
    "#     Spotfire file will be used to visualise the outputs\n",
    "#\n",
    "# Contact: D Robbins\n",
    "#\n",
    "# Global libraries\n",
    "from pyspark.sql import functions as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats, optimize, interpolate\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.linear_model import LinearRegression\n",
    "#\n",
    "# Useful global conversion factors\n",
    "f_bar_to_psi = 14.5037743877082\n",
    "f_m3hr_to_bpd = 150.95544977665\n",
    "f_II_metric_to_field = f_m3hr_to_bpd/f_bar_to_psi\n",
    "#\n",
    "# Steady-state II sampling\n",
    "i_sample_SS = 10   # number of rows for each steady-state II calculation to avoid huge data sets (should be equivalent to number of minutes)\n",
    "#\n",
    "# Bean-up II stampling\n",
    "i_add_previous_row   = 0 # if fewer than this number of P(Q) points in a cluster, add previous PQ value for standard linear regression (0 = will always do this, recommended)\n",
    "i_double_knots       = 0 # if 1, double up knot points for piecewise linear regression (recommended if data points are few), else will reduce number of knots for data size\n",
    "i_want_increasing_II = 1 # if 1, reject II regimes for which injectivity is lower than the previous regime (recommended)\n",
    "#\n",
    "# Useful flags for injector_period_description column (so you can use words, not numbers, in the code body)\n",
    "i_well_offline = 0    # used for PFOs\n",
    "i_well_online = 1     # used for steady-state injection periods\n",
    "i_well_transient = 2  # well performance is changing, unlabelled\n",
    "i_well_beanup = 3     # well performance is changing, well is being beaned up from shut-in\n",
    "i_well_beandown = 4   # well performance is changing, well is being beaned down to shut-in\n",
    "i_well_beanops = 5    # well performance is changing, well is being beaned up/down from non-zero to non-zero rate\n",
    "#\n",
    "# Useful flags for P-Q stabilistaion type pick\n",
    "i_stab_last = 0\n",
    "i_stab_lastclip = 1\n",
    "i_stab_minderv = 2\n",
    "i_stab_mindervclip = 3\n",
    "i_stab_longestdt = 4\n",
    "#\n",
    "# steady-state II flags (to describe PFO length prior to start-up)\n",
    "i_sitype_poor = 0\n",
    "i_sitype_good = 1\n",
    "#\n",
    "# QC scoring\n",
    "i_min_clusters = 4          # number of clusters to be considered a \"good\" bean-up\n",
    "f_min_cluster_power = 1     # qc score related to (1) (clusters/i_min_clusters)^f_min_cluster_power\n",
    "f_CoV_penalty = 0.25        # qc score related to (2) sample coefficient of variation * f_CoV_penalty\n",
    "#\n",
    "# Well dictionary (this needs to be updated regularly)\n",
    "#\n",
    "# indexing for list that dictionary keys refer to\n",
    "i_d_well_RMU = 0                         # RMU that the well belongs to\n",
    "i_d_well_offset = 1                      # Producer(s) that best describes far-field / offset reservoir pressure (as a list, even for a single well)\n",
    "i_d_well_gauge = 2                       # gauge used for well (use \"DHG\" or \"WHG\")\n",
    "i_d_well_status_logic = 3                # method to pick well status (is 1, use rate = 0 as well as IMV/IWV for logic; this can help metered wells)\n",
    "i_d_well_zthresh = 4                     # z-score threshold used to detect choke change events, recommend 2-3 (need higher value for wells with greater choke changes)\n",
    "i_d_well_icv_delta_window = 5            # window to derive the delta ICV over, minutes (recomment 3-5)\n",
    "i_d_well_icv_delta_smooth_window = 6     # rolling period to smooth the delta ICV over, minutes (recomment 3-5)\n",
    "i_d_well_icv_threshold_smooth = 7        # window to smooth the binary choke change over for catching start-end of choke change periods, minutes (recommend 60-120)\n",
    "i_d_well_i_shutin_dt = 8                 # shutin time to take injection back-pressure at for steady-state calculations, hours (recommend 48-120)\n",
    "i_d_well_flow_period = 9                 # length of time (minutes) to consider a bean-up, suggest ~10 mins\n",
    "i_d_well_freq_thresh = 10                # thresholding for rejecting spurious ICV integer thresholding to guess number of k-means clusters, recommend ~5-10\n",
    "i_d_well_cluster_series = 11             # series to perform clustering on; 1 = ICV, 2 = ICV/pressure, 3 = ICV/rate, 4 = ICV/rate/pressure (recommended), 5 = rate/pressure\n",
    "i_d_well_cluster_normalise = 12          # if 1, normalise the series data prior to the clustering; if 0 then use the raw series data for clustering\n",
    "i_d_well_dbscan_PQ_eps = 13              # DBSCAN density clustering parameter for bean-up clustering (recommend ~0.1-0.2 for normalised data, ~2.5-7.5 for non-normalised data on rate/pressure; see PPT!)\n",
    "i_d_well_dbscan_PQ_minsample = 14        # DBSCAN minimum number of samples for bean-up clustering (recommend 2-5)\n",
    "i_d_well_beanup_preferred = 15           # Method picked for reporting back final bean-up P-Q clusters (1 = DBSCAN - recommended, 2 = k-means, 3 = integer ICV)\n",
    "i_d_well_smallest_cluster = 16           # Smallest allowable P(Q) bean-up cluster size (recommend 5 - shorter clusters will be interpreted as noise)\n",
    "i_d_well_monotonicity_method = 17        # Method to estimate monotonicity (0 = min (recommended), 1 = mean, 2 = max, 3 = median)\n",
    "i_d_well_filter_behaviour = 18           # Method to filter poly-monotonic bean-ups (0 = take first zero rate point, 1 = take longest monotonic build (recommended))\n",
    "i_d_well_stabilisation_method = 19       # Method used to pick stabilisation P-Q point (0 = last point, 1 = last point clipped to P10-90 - recommended, 2 = minimum 2D derivative dQ/dt.dP/dt, \n",
    "                                         #   3 = minimum 2D derivative dQ/dt.dP/dt clipped to P10-P90, 4 = longest common dt point)\n",
    "i_d_well_dbscan_II_eps = 20              # injectivity index distance to consider P(Q) point to be in the same injection regime, recommend 10-20 (stb/d/psi)\n",
    "i_d_well_dbscan_II_minsamples = 21       # min samples for II clustering (i.e. must have at least N stabilisation points to consider a flow regime; recommend 1 for wells with few ICV changes otherwise >2)\n",
    "i_d_well_dbscan_discretisation = 22      # method to generate point II for clustering; 0 = first-order backwards (recommended), 1 = first-order backwards to zero point, 2 = instantanous P/Q (not recommended)\n",
    "i_d_well_minfracpressure = 23            # minimum pressure (barg) to be considered a viable frac pressure to avoid spurious temperature dependent II\n",
    "i_d_well_maxII = 24                      # maximum injectivity value (m3/hr/bar) to be considered a viable II (for reference, 100 stb/d/psi = 9.6 m3/hr/bar)\n",
    "i_d_well_maxQfrac = 25                   # maximum frac breakover rate (m3/hr/bar) to be considered a viable II (for reference, 200 m3/hr = 30 mbd)\n",
    "i_d_well_maxPfrac = 26                   # maximum frac breakover pressure (barg) to be considered viable\n",
    "#\n",
    "# dictionary data (example)\n",
    "d_well_data = {\n",
    "    #      |------0-1-------|--2--|------3-8------|----------9-19------------|-------------20-25-------------|\n",
    "    #      |------RMU-------|Gauge|<--Status,PFO->|<--Bean-up clustering---->|--------II cluster/clip--------|\n",
    "    'I01' : ['S1'  ,['P01'] ,'WHG',1,2.5,3,5,90,48,10,10,4,1,0.15,2,1,5,0,1,1,15.0,1,0,100.0,10.0,200.0,275.0],\n",
    "    #                                                    N  EPS m          II m\n",
    "}\n",
    "#\n",
    "# value-to-number dictionary (to fix the Palantir \"feature\" introduced on enumerated data series)\n",
    "d_valve_enumerate = {\n",
    "    \"CLOSED\":[66364536],    # also takes 0, don't see this on GL valves\n",
    "    \"OPEN\":[-1010875264],   # also takes 1, don't see this on GL valves\n",
    "    \"Closed\":[66364536],    # also takes 0, don't see this on GL valves\n",
    "    \"Open\":[-1010875264],   # also takes 1, don't see this on GL valves\n",
    "    \"UNDEFINED\":[255],\n",
    "    \"Undefined\":[-334512128]\n",
    "    }\n",
    "#\n",
    "# global function for interpolating across time series (using scipy.interpolate)\n",
    "#\n",
    "# takes inputs: (1) target x <target df time> (2) defined x <original df time> (3) defined y <original df value>\n",
    "#               (4) interp type ('linear','previous','nearest'; fallback is linear interpolation)\n",
    "# e.g. interpolate_df_col(df['time'],other_df['time'],other_df['col'],'linear')\n",
    "def interpolate_df_col(target_time,def_time,def_y,interp_type):\n",
    "    #\n",
    "    # check interpolation type and assign linear if incorrect value assigned\n",
    "    if (interp_type != 'previous') and (interp_type != 'nearest'):\n",
    "        interp_type = 'linear'\n",
    "    #\n",
    "    # get reference datetime as minimum of two time sets\n",
    "    t_ref = min(min(target_time),min(def_time))\n",
    "    #\n",
    "    # convert target times to floats (days from ref time) so scipy can interpolate\n",
    "    f_def_time = (def_time - t_ref).dt.total_seconds() / 86400.0\n",
    "    f_target_time = (target_time - t_ref).dt.total_seconds() / 86400.0\n",
    "    #\n",
    "    # set up scipy interpolation function (note if we try to interpolate out of bounds, this will set to 0.0)\n",
    "    f = interpolate.interp1d(f_def_time,def_y,kind=interp_type,bounds_error=False,fill_value=0.0)\n",
    "    #\n",
    "    # return the new interpolated array\n",
    "    return f(f_target_time)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_well_data_DHG(Pull_well_data_DHG, list_of_wells):\n",
    "    #\n",
    "    # Script to loop through the well data and do some simple pre-processing\n",
    "    #    e.g. clip outliers, select the main pressure/temperature tag, derive well status etc.\n",
    "    #\n",
    "    #    -> runs on the DHG wells, to remove memory limits on saving dfs <-\n",
    "    #\n",
    "    # Also labels all periods as bean-up, steady-state injection, PFO, bean-down etc.\n",
    "    #    (these correlate to the global code integers i_well_offline, i_well_beanup etc.)\n",
    "    #\n",
    "    df = Pull_well_data_DHG.copy(deep=True)\n",
    "    #\n",
    "    # build list of wells - for now, do this with the available injection wells as we don't have a \"well\" column\n",
    "    # (each name is embedded within the column name)\n",
    "    my_wells = list_of_wells.columns.tolist()\n",
    "    #\n",
    "    for i_df,well in enumerate(my_wells): # we defined the list my_wells above\n",
    "        #\n",
    "        # get the list of column names\n",
    "        col_well_name = well\n",
    "        col_well_IMV = str(well+'_Master_valve')\n",
    "        col_well_IWV = str(well+'_Wing_valve')\n",
    "        col_well_ICV = str(well+'_Choke_valve')\n",
    "        col_well_q = str(well+'_Inj_water_rate')\n",
    "        if d_well_data[well][i_d_well_gauge] == 'DHG':\n",
    "            col_well_p = str(well+'_BHP')\n",
    "            col_well_T = str(well+'_BHT')\n",
    "        else:\n",
    "            col_well_p = str(well+'_WHP')\n",
    "            col_well_T = str(well+'_WHT')\n",
    "        #\n",
    "        # wrap all the building around a try-except clause, so any wells that we didn't pull data for but are still in the \n",
    "        # dictionary (used to build the my_wells list) won't break the code\n",
    "        try:\n",
    "            # build well df\n",
    "            df_well = df[['datetime',col_well_IMV,col_well_IWV,col_well_ICV,col_well_q,col_well_p,col_well_T]].copy(deep=True)\n",
    "            #\n",
    "            # label columns\n",
    "            df_well.columns = ['datetime','IMV','IWV','ICV','rate','pressure','temperature']\n",
    "            #\n",
    "            # insert column for well name\n",
    "            df_well.loc[:,'well'] = [well]*len(df_well)\n",
    "            #\n",
    "            ######################################################################\n",
    "            # 1. Prepare the input data columns (clip outliers, add well status) #\n",
    "            ######################################################################\n",
    "            #\n",
    "            # clip outliers\n",
    "            df_well.loc[:,'rate'] = df_well['rate'].clip(upper=300.0,lower=0.0).replace(300.0,np.nan)                        # note 0 could be a rate here, so don't replace with NaN\n",
    "            df_well.loc[:,'pressure'] = df_well['pressure'].clip(upper=375.0,lower=30.0).replace([375.0,30.0],np.nan)\n",
    "            df_well.loc[:,'temperature'] = df_well['temperature'].clip(upper=70.0,lower=0.0).replace([70.0,0.0],np.nan)\n",
    "            df_well.loc[:,'ICV'] = df_well['ICV'].clip(upper=100.0,lower=0.0).replace([100.0,0.0],np.nan)\n",
    "            #\n",
    "            # drop any row that contains a NaN (should drop clip data)\n",
    "            df_well.dropna(inplace=True)\n",
    "            #\n",
    "            # go through the data and determine well status\n",
    "            # set ICV to -10 if rate is zero as we'll use this to detect bean-ups where only the IWV is opened\n",
    "            well_status = []\n",
    "            icv_status = []\n",
    "            #\n",
    "            # loop over well df and build two lists of (1) well status (2) ICV at current time (overwritten with -10 if well is offline)\n",
    "            #\n",
    "            # option 1 - just use IWV/IMV to derive well status\n",
    "            if d_well_data[well][i_d_well_status_logic] == 0: \n",
    "                #\n",
    "                df_well.loc[:,'status'] = [i_well_online if (x<0 and y<0) else i_well_offline for x,y in zip(df_well['IMV'],df_well['IWV'])]\n",
    "                #\n",
    "            # option 2 - also use rate to derive status (useful for metered wells as additional check)\n",
    "            else:\n",
    "                #\n",
    "                df_well.loc[:,'status'] = [i_well_online if (x<0 and y<0 and z>1.0) else i_well_offline for x,y,z in zip(df_well['IMV'],df_well['IWV'],df_well['rate'])]\n",
    "                #\n",
    "            #\n",
    "            # overwrite ICV if status is zero\n",
    "            df_well.loc[:,'ICV'] = [-10.0 if (stat == i_well_offline) else icv for stat,icv in zip(df_well['status'],df_well['ICV'])]\n",
    "            #\n",
    "            # drop the IMV/IWV data, don't need this any more\n",
    "            df_well.drop(columns=['IMV','IWV'],inplace=True)\n",
    "            #\n",
    "            # sort df by datetime, just to make sure\n",
    "            df_well.sort_values(by='datetime',inplace=True)\n",
    "            #\n",
    "            #########################################################################\n",
    "            # 2. Generate change-of-ICV binary column (to detect periods of change) #\n",
    "            #########################################################################\n",
    "            #\n",
    "            # generate delta ICV column (which is subsequently smoothed - well dictionary contains window to delta/smooth over)\n",
    "            df_well.loc[:,'icv_delta'] = df_well['ICV'].diff(periods=d_well_data[well][i_d_well_icv_delta_window]).rolling(window=d_well_data[well][i_d_well_icv_delta_smooth_window],center=True).mean()\n",
    "            #\n",
    "            # generate z scores of change of ICV (should be strongly centred around ~0 with some noise)\n",
    "            df_well.loc[:,'zscores_delta'] = np.abs(stats.zscore(df_well['icv_delta'].fillna(0)))\n",
    "            #\n",
    "            # threshold on z-scores, i.e. look for outliers (may need to adjust zthresh if well is particularly active, again z clip is stored in well dictionary)\n",
    "            # -> also make sure well is online so we don't detect choke changes from e.g. integrity tests whilst the well is offline\n",
    "            df_well.loc[:,'icv_delta_threshold'] = np.where((df_well.zscores_delta > d_well_data[well][i_d_well_zthresh])&(df_well.status == 1),1,0)\n",
    "            #\n",
    "            # generate a smoothed pick up of the choke change detection (so that we can catch just before a bean-up, for example)\n",
    "            df_well.loc[:,'icv_delta_threshold_smoothed'] = df_well['icv_delta_threshold'].rolling(window=d_well_data[well][i_d_well_icv_threshold_smooth],center=True).mean().fillna(0.0)\n",
    "            #\n",
    "            # finally, create binary map of choke change periods as the smoothing will smear out the 0>1 piecewise function generated in df_well['icv_delta_threshold'] column\n",
    "            df_well.loc[:,'icv_delta_threshold_binary'] = [1 if (icv_thresh > 0.0) else 0 for icv_thresh in df_well['icv_delta_threshold_smoothed']]\n",
    "            #\n",
    "            # drop extra columns now that we have the binary flag (comment these out if you want to save column for debugging)\n",
    "            df_well.drop('zscores_delta',axis=1,inplace=True)\n",
    "            df_well.drop('icv_delta',axis=1,inplace=True)\n",
    "            df_well.drop('icv_delta_threshold',axis=1,inplace=True)\n",
    "            df_well.drop('icv_delta_threshold_smoothed',axis=1,inplace=True)\n",
    "\n",
    "            # drop any rows with NaNs from the dataframe\n",
    "            df_well.dropna(inplace=True)\n",
    "            #\n",
    "            #####################################################################\n",
    "            # 3. Label each period as online, offline, bean-up, bean-down etc.  #\n",
    "            #####################################################################\n",
    "            #\n",
    "            # we will first loop over the threasholded map and increment when we find a new period, whether on or off\n",
    "            # -> this will create a unique list [0,1,2,...,n] where of all periods\n",
    "            # -> we'll label the transient periods in more detail afterwards\n",
    "            #\n",
    "            # initialise loop lists\n",
    "            my_period_ID_list = [0]*len(df_well)           # list of unique period IDs (0,1,2,...,n)\n",
    "            my_period_description_list = [0]*len(df_well)  # will contain integer flag for period type (st-state, bean-up, bean-down, PFO etc.)\n",
    "            my_thresholds = df_well['icv_delta_threshold_binary'].tolist()\n",
    "            my_well_status = df_well['status'].tolist()\n",
    "            #\n",
    "            # initialise  loop / list counter\n",
    "            current_period = 1\n",
    "            prev_thresh = my_thresholds[0]\n",
    "            #\n",
    "            # loop over all periods and label as steady- or unsteady-state\n",
    "            for i,thresh in enumerate(my_thresholds):\n",
    "                #\n",
    "                # (1) injector is in a transient state (bean-up, bean-down, changing operational ICV setting)\n",
    "                if thresh == 1:\n",
    "                    #\n",
    "                    if (prev_thresh == 0) & (i > 0): # if now transient and it wasn't before, increment steady-state counter (ignore first data point in list)\n",
    "                        current_period += 1\n",
    "                    #\n",
    "                    my_period_ID_list[i] = current_period\n",
    "                    my_period_description_list[i] = i_well_transient\n",
    "                    #\n",
    "                #    \n",
    "                # (2) injector is in a steady-state period (online or offline)    \n",
    "                else:\n",
    "                    #\n",
    "                    if (prev_thresh == 1) & (i > 0): # if now steady and the last period was transient, increment the period counter (ignore first data point in list)\n",
    "                        current_period += 1\n",
    "                    #\n",
    "                    my_period_ID_list[i] = current_period\n",
    "                    #\n",
    "                    if my_well_status[i] == 0:\n",
    "                        my_period_description_list[i] = i_well_offline\n",
    "                    else:\n",
    "                        my_period_description_list[i] = i_well_online\n",
    "                #\n",
    "                # update previous thresholded value\n",
    "                prev_thresh = thresh\n",
    "                #\n",
    "            #\n",
    "            # add the period lists to the data frame\n",
    "            df_well.loc[:,'injector_period_ID'] = my_period_ID_list\n",
    "            df_well.loc[:,'injector_period_description'] = my_period_description_list\n",
    "            #\n",
    "            # check all elements have a period ID\n",
    "            if len(df_well[df_well['injector_period_ID']==0]) > 0:\n",
    "                print(str('Warning: '+str(len(df_well[df_well['injector_period_ID']==0]))+' time periods unidentified in well '+well))\n",
    "            #\n",
    "            # set up blank dictionary that we'll map period number to period type on\n",
    "            d_period_update = {}\n",
    "            #\n",
    "            # loop over the transient periods and map transient period to type of period\n",
    "            for inj_period in df_well[df_well['injector_period_description']==i_well_transient]['injector_period_ID'].unique():\n",
    "                #\n",
    "                # get the sub-df for this injection period\n",
    "                temp_df = df_well[df_well['injector_period_ID']==inj_period]\n",
    "                #\n",
    "                # distinguish between bean-up/down and operational choke change by zero rate length (i.e. in the ops one, we're going from finite-to-finite rate)\n",
    "                if len(temp_df[temp_df['rate']<0.1]) > 0:\n",
    "                    #\n",
    "                    # if first rate is zero, well is being beaned up\n",
    "                    # -> this logic will fail if the ICV threshold smoothing window is too short to catch zero rate at start-up\n",
    "                    if temp_df['rate'].iloc[0] < 0.1:\n",
    "                        d_period_update[inj_period] = i_well_beanup\n",
    "                    else:\n",
    "                        d_period_update[inj_period] = i_well_beandown\n",
    "                else:\n",
    "                    # no zero rate points > injector ICV is being changed during normal operations\n",
    "                    d_period_update[inj_period] = i_well_beanops\n",
    "            #        \n",
    "            # go through the periods and replace the injector period description\n",
    "            for inj_period in d_period_update:\n",
    "                df_well.loc[df_well['injector_period_ID']==inj_period,'injector_period_description'] = d_period_update[inj_period]\n",
    "            #\n",
    "            # warn if we have remaining transient periods unlabelled\n",
    "            if len(df_well[df_well['injector_period_description']==i_well_transient])>0:\n",
    "                print(str('Warning: '+str(len(df_well[df_well['injector_period_description']==i_well_transient]))+' transient injection periods are unlabelled in well '+well))\n",
    "            #\n",
    "            # drop the threshold detection column\n",
    "            df_well.drop('icv_delta_threshold_binary',axis=1,inplace=True)\n",
    "            #\n",
    "            ########################################\n",
    "            # 4. Output some useful details to log #\n",
    "            ########################################\n",
    "            #\n",
    "            # print some statistics to the screen\n",
    "            print(str('Data summary for well: '+well))\n",
    "            print(str('  Choke change periods detected: '+str(len(df_well['injector_period_ID'].unique()))))\n",
    "            print(str('    ... of which are bean-ups: '+str(len(df_well[df_well['injector_period_description']==i_well_beanup]['injector_period_ID'].unique()))))\n",
    "            print(str('    ... of which are bean-downs: '+str(len(df_well[df_well['injector_period_description']==i_well_beandown]['injector_period_ID'].unique()))))\n",
    "            print(str('    ... of which are shut-in periods: '+str(len(df_well[df_well['injector_period_description']==i_well_offline]['injector_period_ID'].unique()))))\n",
    "            print(str('    ... of which are online periods: '+str(len(df_well[df_well['injector_period_description']==i_well_online]['injector_period_ID'].unique()))))\n",
    "            print(str('    ... of which are ops change periods: '+str(len(df_well[df_well['injector_period_description']==i_well_beanops]['injector_period_ID'].unique()))))\n",
    "            print(str('    ... of which are unknown transient periods: '+str(len(df_well[df_well['injector_period_description']==i_well_transient]['injector_period_ID'].unique()))))\n",
    "            print('')\n",
    "            #\n",
    "            # output to main df\n",
    "            try :\n",
    "                df_output = pd.concat([df_output,df_well],ignore_index=True)\n",
    "            except:\n",
    "                df_output = df_well.copy(deep=True)\n",
    "        #\n",
    "        # case where we have a well in dictionary but data wasn't pulled, pass this well\n",
    "        except:\n",
    "            pass\n",
    "        #\n",
    "    #\n",
    "    # # replace empty values with NaNs to avoid Apache Parquet dying a premature death\n",
    "    # df_global_ststate_II.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n",
    "    #\n",
    "    return df_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def st_st_II_tracker_DHG(process_well_data_DHG):\n",
    "    #\n",
    "    # Transform to process the steady-state periods and derive injectivity each steady-state point (DHG wells only for memory limits!)\n",
    "    #\n",
    "    # (will take every 10th point to reduce the data base size)\n",
    "    #\n",
    "    # merge together the DHG/WHG input data frames\n",
    "    df = process_well_data_DHG.copy(deep=True)\n",
    "    #\n",
    "    for i_df,well in enumerate(df['well'].unique()):\n",
    "        #\n",
    "        # take sub-slice df of this well\n",
    "        df_well = df[df['well']==well].copy(deep=True)\n",
    "        #\n",
    "        # get list of steady-state and shut-in periods\n",
    "        l_ststate = df_well[df_well['injector_period_description']==i_well_online]['injector_period_ID'].unique().tolist()\n",
    "        l_shutins = df_well[df_well['injector_period_description']==i_well_offline]['injector_period_ID'].unique().tolist()\n",
    "        #\n",
    "        # reset counter for number of processed steady-state periods\n",
    "        i_count_df = 0\n",
    "        #\n",
    "        # loop over the steady-state periods and derive II from each one\n",
    "        for ststate_period in l_ststate:\n",
    "            #\n",
    "            # only bother if the period is >1 into the flow (as we need at least two states - shut-in and bean-up - prior to a st-state period)\n",
    "            if ststate_period > 2:\n",
    "                #\n",
    "                # get the previous periods - must follow a bean-up and a shut-in to be considered a \"good\" steady-state pick\n",
    "                previous_period_1 = df_well[df_well['injector_period_ID']==ststate_period-1]['injector_period_description'].iloc[0]\n",
    "                previous_period_2 = df_well[df_well['injector_period_ID']==ststate_period-2]['injector_period_description'].iloc[0]\n",
    "                #\n",
    "                # if previous two periods are bean-up and offline, analyse the II for this steady-state period\n",
    "                if previous_period_1 == i_well_beanup and previous_period_2 == i_well_offline:\n",
    "                    #\n",
    "                    # glue together the periods and pick zero rate times (as we may have some zero rates during the bean-up period for example)\n",
    "                    df_shutin_period = df_well[df_well['injector_period_ID'].isin([ststate_period-3,ststate_period-2,ststate_period-1])].sort_values(by='datetime')\n",
    "                    df_shutin_period = df_shutin_period[df_shutin_period['status']==i_well_offline]\n",
    "                    #\n",
    "                    # grab the start time and look for an entry n-hours in front\n",
    "                    pfo_start_time = df_shutin_period['datetime'].iloc[0]\n",
    "                    pfo_search_time = pfo_start_time + pd.Timedelta(d_well_data[well][i_d_well_i_shutin_dt],'hours')\n",
    "                    #\n",
    "                    # check if the search time is in the shut-in df by comparing to the search datetime to the last datetime in the df\n",
    "                    if df_shutin_period['datetime'].iloc[-1] < pfo_search_time:\n",
    "                        #\n",
    "                        # if target shut in time not in list (PFO too short), just take the last value as the reservoir pressure\n",
    "                        my_sip = df_shutin_period['pressure'].iloc[-1]   # pressure taken\n",
    "                        my_sit = i_sitype_poor                           # label as poor (inconsistent) pressure\n",
    "                        my_sid = df_shutin_period['datetime'].iloc[-1]   # datetime used for reservoir pressure estimate \n",
    "                    #\n",
    "                    else:\n",
    "                        #\n",
    "                        # if target shut in time is in the list (PFO same/longer than required), take the target PFO shut-in time\n",
    "                        try:\n",
    "                            my_sip = df_shutin_period['pressure'].loc[df_shutin_period['datetime']==pfo_search_time].iloc[0]\n",
    "                            my_sid = pfo_search_time\n",
    "                            my_sit = i_sitype_good\n",
    "                        except: # may fail for some reason, e.g. inconvenient data outage\n",
    "                            my_sip = df_shutin_period['pressure'].iloc[-1]\n",
    "                            my_sid = df_shutin_period['datetime'].iloc[-1]\n",
    "                            my_sit = i_sitype_poor\n",
    "                    #\n",
    "                    # remember the length of the shut in\n",
    "                    my_si_length_d = (max(df_shutin_period['datetime']) - min(df_shutin_period['datetime'])).total_seconds() / 86400.0\n",
    "                    #\n",
    "                    # now, pull the steady-state data\n",
    "                    df_ststate = df_well[df_well['injector_period_ID']==ststate_period]\n",
    "                    #\n",
    "                    # empty lists to store the generated II\n",
    "                    my_II_metric_list = []\n",
    "                    my_II_field_list = []\n",
    "                    my_date_list = []\n",
    "                    #\n",
    "                    # loop over the steady-state data and generate the II with the shut-in pressure pulled above\n",
    "                    for i,row in df_ststate.iterrows():\n",
    "                        #\n",
    "                        # only pull the data on certain samples (i_sample_SS)\n",
    "                        if i % i_sample_SS == 0:\n",
    "                            II_metric = row['rate'] / (row['pressure'] - my_sip)\n",
    "                            my_II_metric_list.append(II_metric)\n",
    "                            my_II_field_list.append(II_metric*f_II_metric_to_field)\n",
    "                            my_date_list.append(row['datetime'])\n",
    "                    #\n",
    "                    # slice df_ststate on the time points picked by the modulo logic\n",
    "                    df_ststate = df_ststate[df_ststate['datetime'].isin(my_date_list)]\n",
    "                    #\n",
    "                    # add II to new dataframe\n",
    "                    df_ststate.loc[:,'II_metric'] = my_II_metric_list\n",
    "                    df_ststate.loc[:,'II_field'] = my_II_field_list\n",
    "                    df_ststate.loc[:,'PRES_shutin'] = [my_sip]*len(df_ststate)\n",
    "                    df_ststate.loc[:,'PRES_type'] = [my_sit]*len(df_ststate)\n",
    "                    df_ststate.loc[:,'PRES_date'] = [my_sid]*len(df_ststate)\n",
    "                    df_ststate.loc[:,'shutin_length_d'] = [my_si_length_d]*len(df_ststate)\n",
    "                    #print(well, ststate_period, my_sid)\n",
    "                    #\n",
    "                    # finally, write st-state df to overall one for this well\n",
    "                    try:\n",
    "                        df_ststate_II = pd.concat([df_ststate_II,df_ststate])\n",
    "                    except:\n",
    "                        df_ststate_II = df_ststate.copy(deep=True)\n",
    "                    #\n",
    "                    # advance the steady-state counter\n",
    "                    i_count_df += 1\n",
    "                    #\n",
    "                # end check for previous period character\n",
    "            # end that st-state period > 2\n",
    "        # end loop over st-state periods for this well\n",
    "        #\n",
    "        # roll up the well st-state II dfs into one global df\n",
    "        try:\n",
    "            df_global_ststate_II = pd.concat([df_global_ststate_II,df_ststate_II],ignore_index=True)\n",
    "        except:\n",
    "            df_global_ststate_II = df_ststate_II.copy(deep=True)\n",
    "        #\n",
    "        # print output message to log\n",
    "        print(str('Processed steady-state injectivity history for well: '+well))\n",
    "        print(str('Total number of II periods: '+str(len(l_ststate))))\n",
    "        print(str())\n",
    "        #\n",
    "    # end loop over wells\n",
    "    #\n",
    "    # replace empty values with NaNs to avoid Apache Parquet dying a premature death\n",
    "    df_global_ststate_II.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n",
    "    #\n",
    "    return df_global_ststate_II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_clusters_DHG(process_well_data_DHG):\n",
    "    #\n",
    "    # Code transform to take the processed bean-up data and determine cluster points for each P(Q) cluster,\n",
    "    #   using the DBSCAN clustering method (https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html)\n",
    "    #\n",
    "    # This is pretty lengthy as there are several cleaning stages applied. Read the PPT \"manual\" if in doubt ...\n",
    "    #\n",
    "    # derive the bean-up data set from the processed df\n",
    "    df = process_well_data_DHG.copy(deep=True).sort_values(by='datetime')\n",
    "    #\n",
    "    for i_df,well in enumerate(df['well'].unique()):\n",
    "        #\n",
    "        # take sub-slice df of this well for the bean-ups only\n",
    "        df_well = df[(df['well']==well) & (df['injector_period_description']==i_well_beanup)].copy(deep=True)\n",
    "        #\n",
    "        # get list of steady-state and shut-in periods\n",
    "        l_beanup_periods = df_well['injector_period_ID'].unique().tolist()\n",
    "        #\n",
    "        # remember the number of bean-up periods\n",
    "        i_no_beanups = len(l_beanup_periods)\n",
    "        #\n",
    "        #############################################################\n",
    "        # 1. Do some early data cleaning on the identified bean-ups #\n",
    "        #    - eliminate periods with too short a flowing time      #\n",
    "        #    - pick longest continuous period if bean-up was        #\n",
    "        #      interrupted                                          #\n",
    "        #############################################################\n",
    "        #\n",
    "        # now, we eliminate any periods with flowing time < n minutes for noisy data\n",
    "        i_flow_period = d_well_data[well][i_d_well_flow_period]\n",
    "        for beanup in l_beanup_periods:\n",
    "            if len(df_well[(df_well['injector_period_ID']==beanup)&(df_well['rate']>0.0)]) < i_flow_period:\n",
    "                l_beanup_periods.remove(beanup)\n",
    "        #\n",
    "        # loop through the long-enough beanups and detect the longest flowing period that defines the main (online) bean-up\n",
    "        for i_df_beanups, beanup in enumerate(l_beanup_periods):\n",
    "            #\n",
    "            # slice the main df on this beanup; reset the index so we can slice on this later\n",
    "            df_beanup = df_well[df_well['injector_period_ID']==beanup].reset_index(drop=True)\n",
    "            #\n",
    "            # initialise beanup loop constants\n",
    "            i_start_from_shutin = 1 # 1 if starts from shut-in state\n",
    "            i_status_changes = 0    # well status will be on or off, that's what we're tracking here for interrupted bean-ups\n",
    "            i_prev_status = df_beanup['status'].iloc[0]  # should equal 1 for the first (1/0 for on/off)\n",
    "            i_period_length = 0\n",
    "            #\n",
    "            # dictionary of sub-periods that we'll build\n",
    "            d_periods = {}\n",
    "            #\n",
    "            # integer slicing for list that dictionary key will refer to\n",
    "            i_d_periods_status = 0\n",
    "            i_d_periods_length = 1\n",
    "            i_d_periods_dfindex = 2\n",
    "            #\n",
    "            # now loop over the rows for this one beanup and build a dictioary of period:[status, length at that status, df index]\n",
    "            for i_dx, row in df_beanup.iterrows():\n",
    "                #\n",
    "                # bean-up should start from shut-in - if not then flag\n",
    "                if (i_dx == 0) and (i_prev_status == 1):\n",
    "                    print(\"Warning - beanup doesn't appear to start from shut-in\")\n",
    "                    print(str(\"Well = \"+well+\"; beanup ID = \"+str(beanup)))\n",
    "                    i_start_from_shutin = 0\n",
    "                #\n",
    "                # count number of periods and length of period\n",
    "                if row['status'] != i_prev_status:\n",
    "                    d_periods[i_status_changes] = [i_prev_status,i_period_length,i_dx] # store in dictionary [previous stat, length at previous stat, row number]\n",
    "                    i_period_length = 0                 # reset period length\n",
    "                    i_status_changes += 1               # increment status change counter\n",
    "                    i_prev_status = row['status']       # update previous status to this row\n",
    "                else:\n",
    "                    i_period_length += 1                # if status is stlil the same, just increment the period length\n",
    "                #\n",
    "            #\n",
    "            # add the last loop entry to the dictionary too\n",
    "            d_periods[i_status_changes] = [i_prev_status,i_period_length,i_dx]\n",
    "            #\n",
    "            # if there are more than two status changes observed in the bean-up (offline > online bean-up) then\n",
    "            #   just take the longest online length and the preceding shut-in\n",
    "            #\n",
    "            # -> we'll do this in a loop, so initialise some loop variables\n",
    "            my_longest_period = 0\n",
    "            len_longest_period = 0\n",
    "            #\n",
    "            # here is the loop - there's probably a more efficient way to do this!\n",
    "            if len(d_periods) > 2:\n",
    "                #\n",
    "                # loop variables\n",
    "                my_longest_period = 0\n",
    "                len_longest_period = 0\n",
    "                #\n",
    "                # loop over each period key in dictionary and find the longest online period\n",
    "                for i_period,period in enumerate(d_periods):\n",
    "                    #\n",
    "                    # if well doesn't start from shut-in, ignore the first flowing period\n",
    "                    if (i_start_from_shutin == 0) and (i_period == 0):\n",
    "                        continue # move on to the next iteration\n",
    "                    else:\n",
    "                        if (d_periods[period][i_d_periods_status] == 1) and (d_periods[period][i_d_periods_length] > len_longest_period):\n",
    "                            my_longest_period = period                                   # update reference to longest period\n",
    "                            len_longest_period = d_periods[period][i_d_periods_length]   # update length of longest period\n",
    "                #\n",
    "                if len_longest_period == 0: # failed to find anything online, whoops\n",
    "                    #\n",
    "                    print('Warning - multiple subperiods present, but failed to find longest')\n",
    "                    print(str(\"Well = \"+well+\"; beanup ID = \"+str(beanup)))\n",
    "                    print('Dictionary of subperiods:')\n",
    "                    print(d_periods)\n",
    "                    df_index_from = 0\n",
    "                    df_index_to = 0\n",
    "                else:\n",
    "                    #\n",
    "                    # otherwise, set df index from longest period and previous one\n",
    "                    #\n",
    "                    # first index for slice is the end of the shut-in index minus the length of that period (to pick start of shut-in)\n",
    "                    #   (need to subtract one because Python slicing is [from-including,to-not-including])\n",
    "                    df_index_from = max(0,d_periods[my_longest_period-1][i_d_periods_dfindex]-d_periods[my_longest_period-1][i_d_periods_length]-1)\n",
    "                    # \n",
    "                    # second index for slice is end point of flowing index\n",
    "                    df_index_to = min(len(df_beanup),d_periods[my_longest_period][i_d_periods_dfindex])\n",
    "                    #\n",
    "            else:\n",
    "                # 2 periods - use all the df\n",
    "                if i_start_from_shutin == 1:\n",
    "                    df_index_from = 0\n",
    "                    df_index_to = len(df_beanup)\n",
    "                # if 2 periods but the first is flowing, slice on nothing (empty df > will be cleansed and ignored)\n",
    "                else:\n",
    "                    df_index_from = 0\n",
    "                    df_index_to = 0\n",
    "            #\n",
    "            # now slice the beanup data-frame on the longest online period\n",
    "            df_beanup = df_beanup.iloc[df_index_from:df_index_to]\n",
    "            #\n",
    "            # save beanup data\n",
    "            if i_df_beanups == 0:\n",
    "                df_beanup_filtered = df_beanup.copy(deep=True)\n",
    "            else:\n",
    "                try:\n",
    "                    df_beanup_filtered = pd.concat([df_beanup_filtered,df_beanup])\n",
    "                except:\n",
    "                    df_beanup_filtered = df_beanup.copy(deep=True)\n",
    "            #\n",
    "        # end loop over beanup to determine longest online period\n",
    "        #\n",
    "        ###########################################################\n",
    "        # 2. Perform clustering on the data; offer ICV threshold, #\n",
    "        #       k-means clustering and DBSCAN (preferred)         #\n",
    "        ###########################################################\n",
    "        #\n",
    "        # first, build list of integer-rounded ICVs\n",
    "        df_beanup_filtered.loc[:,'icv_int'] = df_beanup_filtered['ICV'].astype('int64').tolist()\n",
    "        #\n",
    "        # update the beanup list post-filtering and clean up\n",
    "        l_beanup_periods = df_beanup_filtered['injector_period_ID'].unique()\n",
    "        #\n",
    "        # loop over each beanup again and cluster\n",
    "        for i_df_beanups,beanup in enumerate(l_beanup_periods):\n",
    "            #\n",
    "            # take df of the beanup\n",
    "            df_beanup = df_beanup_filtered[df_beanup_filtered['injector_period_ID'] == beanup].reset_index(drop=True)\n",
    "            #\n",
    "            # reset list for frequency of unique thresholded ICVs\n",
    "            l_ICV_freq = []\n",
    "            #\n",
    "            # determine list of unique integer ICVs and number of df entries for each one (i.e. how many minutes at each position)\n",
    "            for icv in df_beanup['icv_int'].unique():\n",
    "                l_ICV_freq.append([icv,len(df_beanup[df_beanup['icv_int']==icv])])\n",
    "            #\n",
    "            # append list to df of two columns - ICV and number of occurrences- and sort by occurrences\n",
    "            df_ICV_freq = pd.DataFrame(np.array(l_ICV_freq),columns=['icv','thresh']).sort_values(by='thresh')\n",
    "            #\n",
    "            # filter df by threshold of number of occurrences (i.e. length of time at this position)\n",
    "            df_ICV_freq = df_ICV_freq[df_ICV_freq['thresh'] > d_well_data[well][i_d_well_freq_thresh]]\n",
    "            #\n",
    "            # use the length of this thresholded df to estimate number of clusters for other methods (ICV / k-means) downstream\n",
    "            n_clusters = len(df_ICV_freq)\n",
    "            #\n",
    "            # loop dictionary / list initialisation\n",
    "            d_ICV_cluster = {}\n",
    "            my_cluster_number = 0\n",
    "            ICV_cluster_list = []\n",
    "            #\n",
    "            # will build list of thresholded ICV cluster values and assign unique cluster values on this for the beanup (first clustering method)\n",
    "            for icv in df_beanup['icv_int'].tolist():\n",
    "                if icv in d_ICV_cluster: \n",
    "                    # assign existing value to cluster if it's already in the dictionary of unique ICVs\n",
    "                    ICV_cluster_list.append(d_ICV_cluster[icv])      # add the unique cluster number to list previously assigned to this integer ICV\n",
    "                else:\n",
    "                    # if ICV is in the thresholded list, keep the ICV and store value in dictionary\n",
    "                    if icv in df_ICV_freq['icv'].tolist():\n",
    "                        d_ICV_cluster[icv] = my_cluster_number       # assign unique cluster number to map ICV > cluster\n",
    "                        my_cluster_number += 1                       # advance unique cluster number\n",
    "                        ICV_cluster_list.append(d_ICV_cluster[icv])  # add the unique cluster number to list\n",
    "                    #\n",
    "                    # if not in thresholded list, assign a -1 value to label as noise\n",
    "                    else: \n",
    "                        d_ICV_cluster[icv] = -1\n",
    "                        ICV_cluster_list.append(d_ICV_cluster[icv])\n",
    "            #\n",
    "            # store ICV threshold clusters in df\n",
    "            df_beanup['icv_int_clustering'] = ICV_cluster_list\n",
    "            #\n",
    "            # build the input data sets for k-means / DBSCAN clustering methods (both need same style of input)\n",
    "            #\n",
    "            # get the main series data for the clusters\n",
    "            if d_well_data[well][i_d_well_cluster_series] == 1: # ICV only\n",
    "                cluster_data = np.array(df_beanup['ICV'])\n",
    "                cluster_data = np.reshape(cluster_data,(len(cluster_data),1)) # need to reshape 1D array\n",
    "            elif d_well_data[well][i_d_well_cluster_series] == 2: # ICV / pressure\n",
    "                cluster_data = np.array(df_beanup[['ICV','pressure']])\n",
    "            elif d_well_data[well][i_d_well_cluster_series] == 3: # ICV / rate\n",
    "                cluster_data = np.array(df_beanup[['ICV','rate']])\n",
    "            elif d_well_data[well][i_d_well_cluster_series] == 5: # rate / pressure\n",
    "                cluster_data = np.array(df_beanup[['rate','pressure']])\n",
    "            else:                                                 # (4 - default option) ICV/rate/pressure\n",
    "                cluster_data = np.array(df_beanup[['ICV','rate','pressure']])\n",
    "            #\n",
    "            # normalise the data if requested\n",
    "            if d_well_data[well][i_d_well_cluster_normalise] == 1:\n",
    "                cluster_data = StandardScaler().fit_transform(cluster_data)\n",
    "            #\n",
    "            # perform k-means clustering on the data\n",
    "            try:\n",
    "                kmeans = KMeans(n_clusters=n_clusters).fit(cluster_data)\n",
    "                df_beanup.loc[:,'km_cluster_labels'] = kmeans.labels_\n",
    "            except:\n",
    "                df_beanup.loc[:,'km_cluster_labels'] = [-1]*len(df_beanup)\n",
    "            #\n",
    "            # perform DBSCAN clustering on the data\n",
    "            try:\n",
    "                dbscan_cluster = DBSCAN(eps = d_well_data[well][i_d_well_dbscan_PQ_eps], min_samples = d_well_data[well][i_d_well_dbscan_PQ_minsample]).fit(cluster_data)\n",
    "                df_beanup.loc[:,'dbscan_labels'] = dbscan_cluster.labels_\n",
    "            except:\n",
    "                df_beanup.loc[:,'dbscan_labels'] = [-1]*len(df_beanup)            \n",
    "            #\n",
    "            # store the beanup df\n",
    "            if i_df_beanups == 0:\n",
    "                df_clustered = df_beanup.copy(deep=True)\n",
    "            else:\n",
    "                try:\n",
    "                    df_clustered = pd.concat([df_clustered,df_beanup])\n",
    "                except:\n",
    "                    df_clustered = df_beanup.copy(deep=True)\n",
    "            #\n",
    "        # end of loop over beanups to perform clustering\n",
    "        #\n",
    "        ##############################\n",
    "        # 3. Clean up clustered data #\n",
    "        ##############################\n",
    "        #\n",
    "        # set up some loop integers / lists\n",
    "        i_branch_cluster = 0   # use this to refer to list of sub-clusters (dictionary that will be built in the following loop)\n",
    "        i_branch_index = 1     # use this to refer to list of sub-clusters (dictionary that will be built in the following loop)\n",
    "        l_new_clusters = []    # use this to build the new list of cluster values that will be added to df_clustered\n",
    "        #\n",
    "        # set up the preferred cluster column\n",
    "        if d_well_data[well][i_d_well_beanup_preferred] == 1:   # use DBSCAN clusters\n",
    "            df_clustered.loc[:,'cluster'] = df_clustered['dbscan_labels'].tolist()\n",
    "        elif d_well_data[well][i_d_well_beanup_preferred] == 2: # use k-means clusters\n",
    "            df_clustered.loc[:,'cluster'] = df_clustered['km_cluster_labels'].tolist()\n",
    "        else:                                                   # use ICV integer labels\n",
    "            df_clustered.loc[:,'cluster'] = df_clustered['icv_int_clustering'].tolist()\n",
    "        #\n",
    "        # loop over the beanups and clean up clustering - sub-divide those that occur in more than one discrete period\n",
    "        for beanup in df_clustered['injector_period_ID'].unique():\n",
    "            #\n",
    "            # get the beanup data\n",
    "            df_cluster_beanup = df_clustered[df_clustered['injector_period_ID']==beanup].copy(deep=True)\n",
    "            #\n",
    "            # get the unique cluster values for this beanup\n",
    "            l_beanup_cluster = df_cluster_beanup['cluster'].unique()\n",
    "            #\n",
    "            # initialise subdivide cluster number to max of list + 1\n",
    "            i_cluster_subdivide = max(l_beanup_cluster)+1\n",
    "            #\n",
    "            # set up dictionaries for loop within this beanup\n",
    "            d_found_clusters = {}     # dictionary of cluster:row index\n",
    "            d_branched_clusters = {}  # dictionary of cluster:[[sub-cluster-1,sub-index-1],...,[sub-cluster-n,sub-index-n]]\n",
    "            #\n",
    "            # now loop over the df (sort by datetime just to make sure ...) and sub-divide if a second discrete period is found\n",
    "            for i,row in df_cluster_beanup.iterrows():\n",
    "                #\n",
    "                row_cluster = row['cluster']\n",
    "                #\n",
    "                # logic branch if cluster has been found already (ignoring noise points)\n",
    "                if (row_cluster in d_found_clusters) and (row_cluster >= 0):\n",
    "                    #\n",
    "                    # if cluster has been found, check the last cluster element was the entry before\n",
    "                    if d_found_clusters[row_cluster] == i-1:\n",
    "                        #\n",
    "                        d_found_clusters[row_cluster] = i     # if so, update dictionary to reflect this as last element\n",
    "                        l_new_clusters.append(row_cluster)    # store current cluster as correct\n",
    "                        #\n",
    "                    #\n",
    "                    # if it wasn't the last entry, needs to be a new hierarchical cluster\n",
    "                    else:\n",
    "                        # firstly, check if the cluster has already been sub-divided\n",
    "                        if row_cluster in d_branched_clusters:\n",
    "                            #\n",
    "                            # if cluster is sub-branched already, see if one of the sub-branches has the index-1 entry\n",
    "                            i_sub_cluster_identified = 0\n",
    "                            #\n",
    "                            # subclusters are stored as d[row_cluster] = [[sub_cl_1,sub_index_1],[sub_cl_2,sub_index_2],...]\n",
    "                            # dictionary of a list of lists ... my head's exploding\n",
    "                            for i_sub_cluster,l_sub_cluster in enumerate(d_branched_clusters[row_cluster]):\n",
    "                                #\n",
    "                                sub_cluster_value = l_sub_cluster[i_branch_cluster]\n",
    "                                sub_cluster_index = l_sub_cluster[i_branch_index]\n",
    "                                #\n",
    "                                # if the last index was used anywhere, don't need to add a new sub-cluster\n",
    "                                if sub_cluster_index == i-1:\n",
    "                                    #\n",
    "                                    # update index in dictionary\n",
    "                                    d_branched_clusters[row_cluster][i_sub_cluster][i_branch_index] = i\n",
    "                                    # add sub cluster index to list\n",
    "                                    l_new_clusters.append(sub_cluster_value)\n",
    "                                    # update flag to state sub cluster was found\n",
    "                                    i_sub_cluster_identified = 1\n",
    "                                    # don't need to loop any more if we have found the sub-cluster (and run risk of duplicates)\n",
    "                                    break\n",
    "                            #\n",
    "                            # if cluster is not found in existing sub-clusters, write the new sub cluster to the dictionary\n",
    "                            if i_sub_cluster_identified == 0:\n",
    "                                #\n",
    "                                # generate the new sub-cluster label\n",
    "                                i_cluster_subdivide += 1\n",
    "                                # add the cluster to the branch dictionary\n",
    "                                d_branched_clusters[row_cluster] = []\n",
    "                                # append the sub-cluster name and index to the branch dictionary entry\n",
    "                                d_branched_clusters[row_cluster].append([i_cluster_subdivide,i])\n",
    "                                # add the new cluster to list\n",
    "                                l_new_clusters.append(i_cluster_subdivide)\n",
    "                                #\n",
    "                            #\n",
    "                        #\n",
    "                        # if cluster hasn't been sub-divided before, split it here\n",
    "                        else:\n",
    "                            #\n",
    "                            # generate the new sub-cluster label\n",
    "                            i_cluster_subdivide += 1\n",
    "                            # add the cluster to the branch dictionary\n",
    "                            d_branched_clusters[row_cluster] = []\n",
    "                            # append the sub-cluster name and index to the branch dictionary entry\n",
    "                            d_branched_clusters[row_cluster].append([i_cluster_subdivide,i])\n",
    "                            # add the new cluster to list\n",
    "                            l_new_clusters.append(i_cluster_subdivide)\n",
    "                            #\n",
    "                        # if row_cluster in d_branched_clusters\n",
    "                    # if d_found_clusters[row_cluster] == i-1:\n",
    "                #\n",
    "                # logic branch for cluster that hasn't been found already (or is noise)\n",
    "                else:\n",
    "                    if row_cluster < 0:\n",
    "                        # if noise, don't bother to add\n",
    "                        l_new_clusters.append(row_cluster) # store current cluster as correct\n",
    "                    else:\n",
    "                        # if not noise, add cluster to the dictionary and set index to dictionary value\n",
    "                        d_found_clusters[row_cluster] = i\n",
    "                        l_new_clusters.append(row_cluster) # store current cluster as correct\n",
    "                    #\n",
    "                #\n",
    "            # end of loop over bean-up to build list of subclusters\n",
    "        # end of loop over beanups to clean up clusters\n",
    "        #\n",
    "        # append the list of new clusters to the df\n",
    "        df_clustered.loc[:,'new_clusters'] = l_new_clusters\n",
    "        #\n",
    "        # finally, go through the bean-ups and overwrite any small clusters with the noise (-1) flag\n",
    "        i_min_cluster_size = d_well_data[well][i_d_well_smallest_cluster]  # minimum size for cluster\n",
    "        l_filtered_clusters = []                                           # initialise list of filtered cluster values\n",
    "        #\n",
    "        for beanup in df_clustered['injector_period_ID'].unique():\n",
    "            #\n",
    "            # get sliced df for this beanup\n",
    "            df_beanup = df_clustered[df_clustered['injector_period_ID']==beanup]\n",
    "            #\n",
    "            # get the list of clusters\n",
    "            l_clusters = df_beanup['new_clusters'].unique().tolist()\n",
    "            #\n",
    "            # copy the list, Python doesn't like removing elements from iterated lists (fair enough)\n",
    "            #l_clusters_copy = l_clusters.copy() # should work on Python 3.3 onwards, odd\n",
    "            l_clusters_copy = list(l_clusters)   # old Python syntax that works\n",
    "            #\n",
    "            # loop over clusters and remove clusters that are too small\n",
    "            for cluster in l_clusters:\n",
    "                #\n",
    "                # don't remove noise (i.e. only use clusters with zero/positive value)\n",
    "                # also don't remove data with zero rate, regardless of length, as we'll need this for initial pressure guess\n",
    "                if (len(df_beanup[df_beanup['new_clusters'] == cluster]) < i_min_cluster_size) and (cluster >= 0) and (min(df_beanup[df_beanup['new_clusters'] == cluster]['rate'])>0.0):\n",
    "                    l_clusters_copy.remove(cluster)\n",
    "            #\n",
    "            # loop over beanup df and write clusters to list\n",
    "            for i,row in df_beanup.iterrows():\n",
    "                if row['new_clusters'] in l_clusters_copy:\n",
    "                    l_filtered_clusters.append(row['new_clusters'])\n",
    "                else:\n",
    "                    l_filtered_clusters.append(-1)\n",
    "            #   \n",
    "        # end of loop over beanups to clean up small clusters\n",
    "        #\n",
    "        # add the filtered clusters to new column\n",
    "        df_clustered['new_clusters_cleaned'] = l_filtered_clusters\n",
    "        #\n",
    "        ##################################\n",
    "        # 4. Ensure bean-up monotonicity #\n",
    "        ##################################\n",
    "        #\n",
    "        # last point is to loop over the bean-up and ensure monotonicity of ICV; also remove the noise points\n",
    "        # methods that we have for ICV representationL min/ave/max ICV (recommend min ICV for monotonic increase, as if a cluster includes\n",
    "        #   other ICV points with a less-than-optimal n_cluster/EPS then it should still monotonically increase on the next one)\n",
    "        #\n",
    "        # firstly, remove noise points from the df\n",
    "        df_clustered = df_clustered[df_clustered['new_clusters_cleaned'] >= 0]\n",
    "        #\n",
    "        # define ICV measure\n",
    "        i_icv_filter_measure = d_well_data[well][i_d_well_monotonicity_method]\n",
    "        #\n",
    "        # integer flags for filter measure\n",
    "        i_icv_measure_min = 0   # for ICV filter method\n",
    "        i_icv_measure_ave = 1   # for ICV filter method\n",
    "        i_icv_measure_max = 2   # for ICV filter method\n",
    "        i_icv_measure_med = 3   # for ICV filter method\n",
    "        #\n",
    "        # dictionary referencing\n",
    "        i_cluster_min_ICV = 0           # dictionary ref for dictionary of monotonically increasing clusters\n",
    "        i_cluster_start_slice = 1       # dictionary ref for dictionary of monotonically increasing clusters\n",
    "        i_cluster_end_slice = 2         # dictionary ref for dictionary of monotonically increasing clusters\n",
    "        i_cluster_monotonic_cluster = 3 # dictionary ref for dictionary of monotonically increasing clusters\n",
    "        #\n",
    "        # filtering behaviour on more than one monotonic increase\n",
    "        i_icv_filter_behaviour = d_well_data[well][i_d_well_filter_behaviour]\n",
    "        #\n",
    "        # integer flags for filter behaviour\n",
    "        i_cluster_takezero = 0     # logic comparison value\n",
    "        i_cluster_takelongest = 1  # logic comparison value\n",
    "        #\n",
    "        # now loop over df and try to find the clusters that describe monotonic increase the best\n",
    "        for i_df_cluster,beanup in enumerate(df_clustered['injector_period_ID'].unique()):\n",
    "            #\n",
    "            # loop list / counters\n",
    "            d_cluster_monotonics = {}   # dictionary of monotonically increasing clusters (cluster:[icv,length])\n",
    "            icv_previous = 0            # value of previous ICV (reset to zero)\n",
    "            cluster_previous = -10      # value of cluster (reset to arbitrary value that doesn't reflect a DBSCAN/k-means number)\n",
    "            #\n",
    "            # get the beanup df\n",
    "            df_beanup = df_clustered[df_clustered['injector_period_ID']==beanup].reset_index(drop=True)\n",
    "            #\n",
    "            # note that pandas returns unique values in order of appearence, which will be in \n",
    "            #   temporal order for us as we performed sort_values(by='datetime') at the beginning of the transform\n",
    "            for cluster in df_beanup['new_clusters_cleaned'].unique():\n",
    "                #\n",
    "                # build dictionary entry - cluster:[ICV, start index, end index +1 (for Python slicing syntax ...)]\n",
    "                if i_icv_filter_measure == i_icv_measure_min:\n",
    "                    d_cluster_monotonics[cluster] = [min(df_beanup[df_beanup['new_clusters_cleaned']==cluster]['ICV']),df_beanup[df_beanup['new_clusters_cleaned']==cluster].index.values[0],df_beanup[df_beanup['new_clusters_cleaned']==cluster].index.values[-1]+1]\n",
    "                elif i_icv_filter_measure == i_icv_measure_max:\n",
    "                    d_cluster_monotonics[cluster] = [max(df_beanup[df_beanup['new_clusters_cleaned']==cluster]['ICV']),df_beanup[df_beanup['new_clusters_cleaned']==cluster].index.values[0],df_beanup[df_beanup['new_clusters_cleaned']==cluster].index.values[-1]+1]\n",
    "                elif i_icv_filter_measure == i_icv_measure_ave:\n",
    "                    d_cluster_monotonics[cluster] = [np.nanmean(df_beanup[df_beanup['new_clusters_cleaned']==cluster]['ICV']),df_beanup[df_beanup['new_clusters_cleaned']==cluster].index.values[0],df_beanup[df_beanup['new_clusters_cleaned']==cluster].index.values[-1]+1]\n",
    "                else:\n",
    "                    d_cluster_monotonics[cluster] = [np.nanmedian(df_beanup[df_beanup['new_clusters_cleaned']==cluster]['ICV']),df_beanup[df_beanup['new_clusters_cleaned']==cluster].index.values[0],df_beanup[df_beanup['new_clusters_cleaned']==cluster].index.values[-1]+1]\n",
    "                #\n",
    "            #\n",
    "            # loop variables for checking monotonicity\n",
    "            my_icv = -99           # will refer to the ICV picked above that characterises each cluster\n",
    "            i_cluster_loop = 0     # check number of \n",
    "            #\n",
    "            # loop through cluster dictionary and append another value to the list corresponding to the monotonic increase \"subcluster\"\n",
    "            #    (i.e. cluster on monotonicity, starting with 0 (i_cluster_loop))\n",
    "            for i,cluster in enumerate(d_cluster_monotonics):\n",
    "                #\n",
    "                # if ICV is monotonically increasing, append the cluster-of-cluster value to list\n",
    "                if d_cluster_monotonics[cluster][i_cluster_min_ICV] > my_icv: # note we are not taking the min ICV, this is just a handy reference to the 0th element in list (ICV)\n",
    "                    d_cluster_monotonics[cluster].append(i_cluster_loop)      # add to the dictionary list above, i.e. list slice has gone from range(0,3) to range(0,4)\n",
    "                    my_icv = d_cluster_monotonics[cluster][i_cluster_min_ICV]\n",
    "                # if monotonicity of ICV is not observed, forward the monotonic subcluster counter and append this to list\n",
    "                else:\n",
    "                    i_cluster_loop += 1\n",
    "                    d_cluster_monotonics[cluster].append(i_cluster_loop)\n",
    "                    my_icv = d_cluster_monotonics[cluster][i_cluster_min_ICV]   \n",
    "            #\n",
    "            # make a df out of the monotonic subclustering dictionary to make life a little easier\n",
    "            df_subcluster = pd.DataFrame.from_dict(d_cluster_monotonics,orient='index',columns=['icv','from_index','to_index','subcluster'])\n",
    "            #\n",
    "            # determine which cluster to take - initialise values\n",
    "            i_mymax_cluster = 0   # initialise value of monotonic cluster prior to logic below\n",
    "            #\n",
    "            # now, define which clusters to take forward on the P(Q) plot based on monotonic filter behaviour specified in well dictionary\n",
    "            if i_icv_filter_behaviour == i_cluster_takezero:\n",
    "                # option 1 - take the one that starts at zero rate (should be the first)\n",
    "                # (could improve this logic to look through all monotonic clusters for longest that starts at zero rate,\n",
    "                #    but we already scrubbed the data so we are only looking at one continuous flowing period)\n",
    "                i_mymax_cluster = df_subcluster['subcluster'].iloc[0]\n",
    "            else:\n",
    "                # loop over the clusters and work out how many rows are in each monotonic cluster, then take the longest\n",
    "                d_cluster_sum = {}\n",
    "                # first, generate a dictionary of montonoic cluster : number of df rows\n",
    "                for ind,row in df_subcluster.iterrows():\n",
    "                    i_cluster = row['subcluster']\n",
    "                    if i_cluster in d_cluster_sum:\n",
    "                        d_cluster_sum[i_cluster] += row['to_index']-row['from_index']\n",
    "                    else:\n",
    "                        d_cluster_sum[i_cluster] = row['to_index']-row['from_index']\n",
    "                    #\n",
    "                #\n",
    "                # now loop over these and find the largest monotonic subcluster\n",
    "                i_max_cluster = 0     # counter for total number of rows in cluster\n",
    "                for cluster in d_cluster_sum:\n",
    "                    if d_cluster_sum[cluster] > i_max_cluster:\n",
    "                        i_mymax_cluster = cluster\n",
    "                        i_max_cluster = d_cluster_sum[cluster]\n",
    "                #\n",
    "            # finally, get the slice integers (remember the \"to\" already has the Python +1 needed)\n",
    "            i_slice_from = df_subcluster[df_subcluster['subcluster']==i_mymax_cluster]['from_index'].iloc[0]\n",
    "            i_slice_to = df_subcluster[df_subcluster['subcluster']==i_mymax_cluster]['to_index'].iloc[-1]\n",
    "            #\n",
    "            # finally, slice the df to give a monotonically increasing P(Q) data set\n",
    "            df_beanup = df_beanup.iloc[i_slice_from:i_slice_to]\n",
    "            #\n",
    "            # if we only have one cluster left, skip adding this cluster to the monotonic cluster list\n",
    "            if len(df_beanup['new_clusters_cleaned'].unique()) < 2:\n",
    "                continue\n",
    "            #\n",
    "            # build the final df\n",
    "            if i_df_cluster == 0:\n",
    "                df_clustered_monotonic = df_beanup.copy(deep=True)\n",
    "            else:\n",
    "                try:\n",
    "                    df_clustered_monotonic = pd.concat([df_clustered_monotonic,df_beanup])\n",
    "                except:\n",
    "                    # above would fail if we removed the first bean-up from the data set and thus df_clustered_monotonic didn't exist, this should get around this\n",
    "                    # (you could also just initialise df_clustered_monotonic with the df_beanup columns, but meh)\n",
    "                    df_clustered_monotonic = df_beanup.copy(deep=True)\n",
    "            #\n",
    "        # end loop over bean-ups to enforce monotonicity of bean-up clusters\n",
    "        #\n",
    "        # print some stats\n",
    "        print(str('Bean-up cleaning and clustering summary for well '+well+':'))\n",
    "        print(str('  pre-filtering bean-ups: '+str(i_no_beanups)))\n",
    "        print(str('  post-filtered, clustered and cleaned bean-ups: '+str(len(df_clustered_monotonic['injector_period_ID'].unique()))))\n",
    "        print('')\n",
    "        #\n",
    "        # compile the interpreted well bean-ups\n",
    "        if i_df == 0:\n",
    "            df_final_beanup = df_clustered_monotonic.copy(deep=True) \n",
    "        else:\n",
    "            try:\n",
    "                df_final_beanup = pd.concat([df_final_beanup,df_clustered_monotonic],ignore_index=True)\n",
    "            except:\n",
    "                df_final_beanup = df_clustered.copy(deep=True) \n",
    "        #\n",
    "    # end loop over each well\n",
    "    #\n",
    "    # drop/rename some columns that are no longer useful (e.g. injector period, they'll all be beanups here)\n",
    "    df_final_beanup.drop('icv_int',axis=1,inplace=True)\n",
    "    df_final_beanup.drop('injector_period_description',axis=1,inplace=True)\n",
    "    df_final_beanup.drop('cluster',axis=1,inplace=True)\n",
    "    df_final_beanup.drop('new_clusters',axis=1,inplace=True)\n",
    "    df_final_beanup.rename(columns={'new_clusters_cleaned':'cluster'},inplace=True)\n",
    "    #\n",
    "    return df_final_beanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beanup_QC(merged_beanup_clusters):\n",
    "    #\n",
    "    # short transform that goes through the bean-ups post-cleaning/clustering and\n",
    "    #   gives a QC score that looks at number of steps, length per step etc.\n",
    "    #\n",
    "    #df = pd.concat([pick_clusters_DHG,pick_clusters_WHG],sort=False,ignore_index=True)\n",
    "    df = merged_beanup_clusters.copy(deep=True)\n",
    "    #\n",
    "    # initialise df and list\n",
    "    df_qc = pd.DataFrame(columns=['well','injector_period_ID','beanup_qc'])\n",
    "    #\n",
    "    for well in df['well'].unique():\n",
    "        #\n",
    "        # take sub-slice df of this well\n",
    "        df_well = df[df['well']==well]\n",
    "        #\n",
    "        # loop over beanups and score between 0 (bad) and 1 (good)\n",
    "        for beanup in df_well['injector_period_ID'].unique():\n",
    "            #\n",
    "            # reset QC score\n",
    "            f_qc_score = 1.0\n",
    "            #\n",
    "            # get the sub-sliced df\n",
    "            df_beanup = df_well[df_well['injector_period_ID']==beanup]\n",
    "            #\n",
    "            # adjust qc score for number of clusters (if > min clusters, don't penalise score, i.e. 1 - 1 is subtracted from QC score)\n",
    "            f_qc_score -= 1.0 - min(1.0,(len(df_beanup['cluster'].unique())/i_min_clusters)**f_min_cluster_power)\n",
    "            #\n",
    "            # qc score for variance of cluster length (i.e. time spent within each cluster)\n",
    "            #\n",
    "            # build list of cluster lengths (note we cleaned up the noisy data, so each cluster should have value > -1)\n",
    "            l_cluster_length = []   # list containing length of time at each cluster\n",
    "            for cluster in df_beanup['cluster'].unique():\n",
    "                l_cluster_length.append(len(df_beanup[df_beanup['cluster']==cluster]))\n",
    "            #\n",
    "            # measure the variability by the sample coefficient of variation for list, which is a nice normalised measure of dispersion (0 = perfect equivalent times at each step)\n",
    "            #  (note this needs at least two clusters, otherwise np.std will divide by 0; cluster cleaning/filtering SHOULD guarantee this, but set hard penalty if it hasn't)\n",
    "            try:\n",
    "                f_CoV = (np.std(l_cluster_length,ddof=1)/np.mean(l_cluster_length))*f_CoV_penalty\n",
    "            except:\n",
    "                f_CoV = 1.0\n",
    "            #\n",
    "            # apply qc penalty\n",
    "            f_qc_score -= f_CoV\n",
    "            #\n",
    "            # check qc is in bound [0,1]\n",
    "            f_qc_score = min(max(f_qc_score,0.0),1.0)\n",
    "            #\n",
    "            # add data to df (remember double square brackets on list data, otherwise Pandas tries to make things nxn rather than 1xn size ...)\n",
    "            df_qc = pd.concat([df_qc,pd.DataFrame([[well,beanup,f_qc_score]],columns=['well','injector_period_ID','beanup_qc'])],ignore_index=True)\n",
    "            #\n",
    "        # end loop over bean-ups\n",
    "        print(str('Completed bean-up QC for well '+well))\n",
    "    # end loop over wells \n",
    "    #\n",
    "    # return the qc df\n",
    "    return df_qc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beanup_PQ_stabilisation(merged_beanup_clusters):\n",
    "    #\n",
    "    # Transform that loops through the cleaned-up bean-up data and picks a single pressure and rate\n",
    "    #   point per cluster / point of the bean-up\n",
    "    #\n",
    "    # This will allow us to do the final II / frac pressure picks, rather than using the whole data \n",
    "    #   set to do this\n",
    "    #\n",
    "    df = merged_beanup_clusters.copy(deep=True)\n",
    "    #\n",
    "    # initialise list to build final df from\n",
    "    l_stab_points = []\n",
    "    #\n",
    "    for well in df['well'].unique():\n",
    "        #\n",
    "        # take sub-slice df of this well\n",
    "        df_well = df[df['well']==well].copy(deep=True).sort_values(by='datetime')\n",
    "        #\n",
    "        # loop over beanups and derive P(Q) for each beanup\n",
    "        for beanup in df_well['injector_period_ID'].unique():\n",
    "            #\n",
    "            # get beanup df to shorten typing required ...\n",
    "            df_beanup = df_well[df_well['injector_period_ID']==beanup]\n",
    "            #\n",
    "            # (1) find the shortest cluster in the beanup and determine its length\n",
    "            #\n",
    "            i_shortest_cluster = 1000000\n",
    "            for cluster in df_beanup['cluster'].unique():\n",
    "                #\n",
    "                # subslice df (makes the code more legible ...)\n",
    "                df_cluster = df_beanup[df_beanup['cluster'] == cluster]\n",
    "                #\n",
    "                # skip update cluster length measure if rates are zero at this point\n",
    "                if len(df_cluster[df_cluster['rate']<0.1]) > 0:\n",
    "                    continue\n",
    "                else:\n",
    "                    if len(df_cluster) < i_shortest_cluster+1:\n",
    "                        # recall that iloc slices on index, length will be index +1 ...\n",
    "                        i_shortest_cluster = len(df_cluster)-1\n",
    "                #\n",
    "            #\n",
    "            # if we still have the same value, just override with last value\n",
    "            if i_shortest_cluster == 1000000:\n",
    "                i_shortest_cluster = len(df_beanup['cluster'].unique()[-1]) - 1\n",
    "            #\n",
    "            # (2) determine the clip of the last beanup (as long as beanup has more than 2 clusters ...)\n",
    "            #     -> this will make sure that a final bean-up step will not take into account long pressure build, i.e.\n",
    "            #        we will clip the last stage of the bean-up based on the length of previous steps of the bean-up\n",
    "            #\n",
    "            if (len(df_beanup['cluster'].unique())>2):\n",
    "                #\n",
    "                i_sum_cluster = 0\n",
    "                i_count_cluster = 0\n",
    "                #\n",
    "                for i_cluster,cluster in enumerate(df_beanup['cluster'].unique()):\n",
    "                    #\n",
    "                    # subslice df (makes the code more legible ...)\n",
    "                    df_cluster = df_beanup[df_beanup['cluster'] == cluster]\n",
    "                    #\n",
    "                    # make sure this isn't the last cluster and isn't zero rate\n",
    "                    if (i_cluster < len(df_beanup['cluster'].unique())-1) and (len(df_cluster[df_cluster['rate']<0.1]) == 0):\n",
    "                        #\n",
    "                        # count the number of points in the cluster\n",
    "                        i_sum_cluster += len(df_cluster)-1\n",
    "                        i_count_cluster += 1\n",
    "                    #\n",
    "                # determine the last cluster clip from mean of the cluster sizes\n",
    "                try:\n",
    "                    i_last_cluster_clip = int(i_sum_cluster/i_count_cluster)-2\n",
    "                except:\n",
    "                    i_last_cluster_clip = len(df_beanup[df_beanup['cluster'] == df_beanup['cluster'].unique()[-1]])-1\n",
    "                #\n",
    "            else:\n",
    "                # just use the length of the last cluster otherwise\n",
    "                i_last_cluster_clip = len(df_beanup[df_beanup['cluster'] == df_beanup['cluster'].unique()[-1]])-1\n",
    "            #\n",
    "            # finally, reset the last cluster clip based on its own size if needed\n",
    "            i_last_cluster_clip = min(i_last_cluster_clip,len(df_beanup[df_beanup['cluster'] == df_beanup['cluster'].unique()[-1]])-1)\n",
    "            #\n",
    "            # (3)  loop over clusters and determine the stabilisation pick\n",
    "            #\n",
    "            for i_cluster,cluster in enumerate(df_beanup['cluster'].unique()):\n",
    "                #\n",
    "                # reset stabilisation values\n",
    "                f_stab_p = 0.0\n",
    "                f_stab_q = 0.0\n",
    "                #\n",
    "                # get df (again ... just keeps the line length down, should really just consider rolling up loops over bean-ups)\n",
    "                df_cluster = df_beanup[df_beanup['cluster'] == cluster].reset_index(drop=True)\n",
    "                #\n",
    "                # for the representative temperature, just take the first temperature point rather than the average\n",
    "                # (could add an option to control this, but I think it's a little overboard as really we care about initial and \n",
    "                #    frac onset temperature, which both happen at the \"start\" of a cluster)\n",
    "                f_cluster_T = df_cluster['temperature'].iloc[0]\n",
    "                #\n",
    "                # if the cluster is the last one, we clip it to prevent a long pressure build (different transient) influencing P(Q) using the\n",
    "                #    index derived above in (2)\n",
    "                #\n",
    "                if (i_cluster == len(df_beanup['cluster'].unique())-1):\n",
    "                    #\n",
    "                    # clip df (remember Python slices from:to-but-not-including!)\n",
    "                    # (think I've doubled up and done -1+1 = 0 here, but I'm now too scared to get rid of them as it works)\n",
    "                    df_cluster = df_cluster.iloc[0:i_last_cluster_clip+1]\n",
    "                    #\n",
    "                    # also override the shortest pick if necessary for i_stab_pick == i_stab_longestdt case\n",
    "                    i_shortest_cluster = min(i_shortest_cluster,i_last_cluster_clip)\n",
    "                    #\n",
    "                #\n",
    "                # if cluster has zero rate in it, we take the last point as its stabilisation regardless of method\n",
    "                #\n",
    "                # set up initial pressure point, we'll overwrite below\n",
    "                f_stab_p0 = df_cluster['pressure'].iloc[0]\n",
    "                #\n",
    "                if len(df_cluster[df_cluster['rate']<1.0]) > 0:\n",
    "                    #\n",
    "                    # find the last point at zero rate\n",
    "                    i_zero_rate = 0\n",
    "                    for i,rate in enumerate(df_cluster['rate'].tolist()):\n",
    "                        if rate < 1.0:\n",
    "                            i_zero_rate = i\n",
    "                        #\n",
    "                    # return the p,q stabilisation point\n",
    "                    f_stab_p1 = df_cluster['pressure'].tolist()[i_zero_rate]\n",
    "                    f_stab_q1 = 0.0\n",
    "                    f_stab_p2 = df_cluster['pressure'].tolist()[i_zero_rate]\n",
    "                    f_stab_q2 = 0.0\n",
    "                    f_stab_p3 = df_cluster['pressure'].tolist()[i_zero_rate]\n",
    "                    f_stab_q3 = 0.0\n",
    "                    f_stab_p4 = df_cluster['pressure'].tolist()[i_zero_rate]\n",
    "                    f_stab_q4 = 0.0\n",
    "                    f_stab_p5 = df_cluster['pressure'].tolist()[i_zero_rate]\n",
    "                    f_stab_q5 = 0.0\n",
    "                    #\n",
    "                    # also record zero-point pressure here as we'll use this to condition the local stress state\n",
    "                    f_stab_p0 = df_cluster['pressure'].tolist()[i_zero_rate]\n",
    "                    #\n",
    "                else:\n",
    "                    #\n",
    "                    # pick the stabilised pressure-rate point on the cross-plot for this cluster\n",
    "                    #   (we'll record all methods and then select in the II determination)\n",
    "                    #\n",
    "                    # (1) take the last point in the cluster\n",
    "                    f_stab_p1 = df_cluster['pressure'].iloc[-1]\n",
    "                    f_stab_q1 = df_cluster['rate'].iloc[-1]\n",
    "                    #\n",
    "                    # (2) take the last point, but clip to 10-90 percentile of time (for choke change data)\n",
    "                    clip_from = max(0,int(len(df_cluster)*0.1))\n",
    "                    clip_to = min(len(df_cluster),int(len(df_cluster)*0.9))\n",
    "                    #\n",
    "                    # report back last point of the clipped series for rate and pressre\n",
    "                    f_stab_p2 = df_cluster['pressure'].iloc[clip_from:clip_to].iloc[-1]\n",
    "                    f_stab_q2 = df_cluster['rate'].iloc[clip_from:clip_to].iloc[-1]\n",
    "                    #\n",
    "                    # (3) take the point at which (dP/dt,dQ/dt) is at a minimum\n",
    "                    #\n",
    "                    # compute the derivative values\n",
    "                    # we can cheat here as the delta time is the same between each row thanks to interpolation (probably)\n",
    "                    #  (I could be more thorough and build in a Timedelta column to calculate the actual dt, but meh)\n",
    "                    l_dp_dt = df_cluster['pressure'].diff().iloc[1:len(df_cluster)].abs()\n",
    "                    l_dq_dt = df_cluster['rate'].diff().iloc[1:len(df_cluster)].abs()\n",
    "                    #\n",
    "                    # sum the two series (take absolute value - done above) and then do a simple lookup to find the stab point\n",
    "                    l_sum_dt = l_dp_dt.add(l_dq_dt,fill_value=10000000.0)\n",
    "                    i_stab_point = l_sum_dt.loc[l_sum_dt==min(l_sum_dt)].index.values[0]  # should only return a single value ... (i.e. should pick first if we have two identical minima)\n",
    "                    #\n",
    "                    # report back the stabilisation point\n",
    "                    f_stab_p3 = df_cluster['pressure'].iloc[i_stab_point]\n",
    "                    f_stab_q3 = df_cluster['rate'].iloc[i_stab_point]\n",
    "                    #\n",
    "                    # (4) take the point at which (dP/dt,dQ/dt) is at a minimum, with P10-90 clip applied\n",
    "                    # same as above, but clip to P10-90 of the cluster\n",
    "                    #\n",
    "                    # compute the derivative values\n",
    "                    # we can cheat here as the delta time is the same between each row thanks to interpolation\n",
    "                    #\n",
    "                    clip_from = max(1,int(len(df_cluster)*0.1))             # P10 clip extent (overwrite 0 with 1 here so we don't include the first NaN in diff() series)\n",
    "                    clip_to = min(len(df_cluster),int(len(df_cluster)*0.9)) # P90 clip extent\n",
    "                    l_dp_dt = df_cluster['pressure'].diff().iloc[clip_from:clip_to].abs()\n",
    "                    l_dq_dt = df_cluster['rate'].diff().iloc[clip_from:clip_to].abs()\n",
    "                    #\n",
    "                    # sum the two series (take absolute value!) and then do a simple lookup to find the stab point\n",
    "                    l_sum_dt = l_dp_dt.add(l_dq_dt,fill_value=10000000.0)\n",
    "                    i_stab_point = l_sum_dt.loc[l_sum_dt==min(l_sum_dt)].index.values[0]  # should be OK for a single value ...\n",
    "                    #\n",
    "                    # report back the stabilisation point\n",
    "                    f_stab_p4 = df_cluster['pressure'].iloc[i_stab_point]\n",
    "                    f_stab_q4 = df_cluster['rate'].iloc[i_stab_point]\n",
    "                    #\n",
    "                    # (5) take consistent dt transient\n",
    "                    # report back the stabilisation point at the same dt (based on shortest cluster)\n",
    "                    f_stab_p5 = df_cluster['pressure'].iloc[i_shortest_cluster]\n",
    "                    f_stab_q5 = df_cluster['rate'].iloc[i_shortest_cluster]\n",
    "                # (end if statement if cluster has zero rate)\n",
    "                #\n",
    "                # build df list - we'll have a row per stabilisation type so all data is available for debugging or \n",
    "                #   choosing the best stabilisation method for the well\n",
    "                l_stab_points.append([well,beanup,cluster,i_stab_last,f_stab_p0,f_stab_p1,f_stab_q1,f_cluster_T])\n",
    "                l_stab_points.append([well,beanup,cluster,i_stab_lastclip,f_stab_p0,f_stab_p2,f_stab_q2,f_cluster_T])\n",
    "                l_stab_points.append([well,beanup,cluster,i_stab_minderv,f_stab_p0,f_stab_p3,f_stab_q3,f_cluster_T])\n",
    "                l_stab_points.append([well,beanup,cluster,i_stab_mindervclip,f_stab_p0,f_stab_p4,f_stab_q4,f_cluster_T])\n",
    "                l_stab_points.append([well,beanup,cluster,i_stab_longestdt,f_stab_p0,f_stab_p5,f_stab_q5,f_cluster_T])\n",
    "            # (end loop over clusters)\n",
    "        # (end loop over beanups)\n",
    "        print(str('Completed beanup p-q picks for well '+well))\n",
    "    # (end loop over wells)\n",
    "    #\n",
    "    # build df from detected stabilisation points\n",
    "    df_pq = pd.DataFrame(l_stab_points,columns=['well','injector_period_ID','cluster','stab_type','pressure_initial','pressure','rate','temperature'])\n",
    "    #\n",
    "    return df_pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beanup_II_tracker(beanup_PQ_stabilisation):\n",
    "    #\n",
    "    # Transform that loops through the P(Q) picks for each bean-up and derives\n",
    "    #   injectivity index, frac pressure etc. for the well\n",
    "    #\n",
    "    # Loops over each bean-up and assigns point to II clusters based on local dP/dQ gradient\n",
    "    # Methods used: (1) simple averaging over II clusters, (2) linear regression over II clusters, (3) piecewise linear regression over all data\n",
    "    #\n",
    "    df = beanup_PQ_stabilisation.copy(deep=True)\n",
    "    #\n",
    "    # set up final injectivity df\n",
    "    df_injectivity = pd.DataFrame(columns=['well','injector_period_ID','II_measure','flow_regime','II_field','II_metric','intercept_field','intercept_metric','Pfrac_field','Pfrac_metric','Qfrac_field','Qfrac_metric'])\n",
    "    #\n",
    "    for i_df,well in enumerate(df['well'].unique()):\n",
    "        #\n",
    "        # take sub-slice df of this well\n",
    "        df_well = df[df['well']==well]\n",
    "        #\n",
    "        # loop over beanups and derive injectivity and frac pressure for each beanup\n",
    "        for beanup in df_well['injector_period_ID'].unique(): \n",
    "            #\n",
    "            # get the beanup df\n",
    "            df_beanup = df_well[df_well['injector_period_ID']==beanup].copy(deep=True)#.reset_index(drop=True)\n",
    "            #\n",
    "            # sub-slice this df based on P(Q) stabilisation method (reset the index so we can do proper calling of df indices 0,1,2,...,n)\n",
    "            df_beanup = df_beanup[df_beanup['stab_type'] == d_well_data[well][i_d_well_stabilisation_method]].reset_index(drop=True)\n",
    "            #\n",
    "            # check that we have a unique cluster data set, warn if we don't\n",
    "            if len(df_beanup) != len(df_beanup['cluster'].unique()):\n",
    "                print(str('Warning: cluster set for bean-up II derivation is non-unique ... continuing with caution'))\n",
    "                print(df_beanup.head(10))\n",
    "            #\n",
    "            ############################################################################################\n",
    "            # (1) calculate injectivity with II clustering (don't use frac pressures, derive a priori) #\n",
    "            ############################################################################################\n",
    "            #\n",
    "            # calculate the raw dp/dt values\n",
    "            l_ii_grad = []\n",
    "            q_prev = 0.0\n",
    "            p_prev = 0.0\n",
    "            q_0 = df_beanup['rate'].iloc[0]\n",
    "            p_0 = df_beanup['pressure'].iloc[0]\n",
    "            #\n",
    "            # append II values to the beanup df (remember - this is the inverse of the P(Q) plot gradient)\n",
    "            for i,r in df_beanup.iterrows():\n",
    "                #\n",
    "                try:\n",
    "                    # ignore first row data - np.nan will be ignored by cluster data\n",
    "                    if i == 0:\n",
    "                        l_ii_grad.append(np.nan)\n",
    "                    #\n",
    "                    # otherwise generate the guess of II for this point (we'll guess field-unit II, which is actaully the inverse of the PQ plot)\n",
    "                    else:\n",
    "                        # method 1, use immediate first-order backwards derivative\n",
    "                        if d_well_data[well][i_d_well_dbscan_discretisation] == 0:\n",
    "                            l_ii_grad.append(((r['rate'] - q_prev)/(r['pressure'] - p_prev))*f_II_metric_to_field)\n",
    "                        #\n",
    "                        # method 2, use total first-order backwards derivative\n",
    "                        elif d_well_data[well][i_d_well_dbscan_discretisation] == 1:\n",
    "                            l_ii_grad.append(((r['rate'] - q_0)/(r['pressure'] - p_0))*f_II_metric_to_field)\n",
    "                        #\n",
    "                        # method 3, use instantaneous P/Q (you mad man/woman)\n",
    "                        else:\n",
    "                            l_ii_grad.append((r['rate']/r['pressure'])*f_II_metric_to_field)\n",
    "                except:\n",
    "                    l_ii_grad.append(0.0)\n",
    "                #\n",
    "                # update previous guesses\n",
    "                q_prev = r['rate']\n",
    "                p_prev = r['pressure']\n",
    "            #\n",
    "            # append the estimated II to the beanup df\n",
    "            df_beanup.loc[:,'ii_gradient'] = l_ii_grad\n",
    "            #\n",
    "            ##############################################################\n",
    "            # (2) use DBSCAN to generate clusters of similar injectivity #\n",
    "            ##############################################################\n",
    "            #\n",
    "            # Set up the epsilon parameter. This controls max distance to be considered in same centroid;\n",
    "            #    typically frac mode is >5-10 stb/d/psi different so use this as a starting point.\n",
    "            #    - you'll want to do some sensitivity study for each well to see II(t,EPS)\n",
    "            #    - also bear in mind we may have initially high II on long shut in bean ups (higher BHT > lower viscosity water); it's the DIFFERENCE between regimes we care about\n",
    "            #    => too high a value = everything is considered to be the same flow  regime\n",
    "            #    => too low a value = everything is considered to be in a different flow regime\n",
    "            f_dbscan_eps = d_well_data[well][i_d_well_dbscan_II_eps]\n",
    "            f_dbscan_minsamples = d_well_data[well][i_d_well_dbscan_II_minsamples]\n",
    "            #\n",
    "            # find and store rows with NaN in II (won't use these to cluster, but we'll add them back in at the end)\n",
    "            # these could be (1) first row values (backwards II from above will have NaN for first value) (2) values where P is identical between subsequent points\n",
    "            my_nan_indices = []\n",
    "            my_nan_rows = []\n",
    "            for i,r in df_beanup.iterrows():\n",
    "                if np.isnan(r['ii_gradient']):\n",
    "                    my_nan_indices.append(i)\n",
    "                    my_nan_rows.append(r.values)\n",
    "            #\n",
    "            # drop these rows from the df\n",
    "            df_beanup = df_beanup.iloc[[i for i in df_beanup.index.tolist() if i not in my_nan_indices]]\n",
    "            #\n",
    "            # get the 1D data (II) to cluster on and get it into np array so we can run DBSCAN\n",
    "            cluster_data = np.array(df_beanup['ii_gradient'].dropna())     # the .iloc[] logic above SHOULD remove all NaNs, so the dropna() is just in case\n",
    "            cluster_data = np.reshape(cluster_data,(len(cluster_data),1))  # DBSCAN wants an exactly 1D array for input ...\n",
    "            #\n",
    "            # generate the clusters\n",
    "            dbscan_cluster = DBSCAN(eps=f_dbscan_eps,min_samples=f_dbscan_minsamples).fit(cluster_data)\n",
    "            #\n",
    "            # build the cluster list, note first data point doesn't have II computed so label as noise (-1 in cluster parlance)\n",
    "            l_dbscan_cluster = dbscan_cluster.labels_.tolist()\n",
    "            #\n",
    "            # update cluster labelling to include any points of the II gradient with no cluster value (usually the first point)\n",
    "            for index in my_nan_indices:\n",
    "                try:\n",
    "                    # insert the value and use cluster label that is the next point in the list\n",
    "                    l_dbscan_cluster.insert(index,l_dbscan_cluster[index])\n",
    "                except:\n",
    "                    # if this fails, index must be greater than the df, so use the value of the last one and insert at the end\n",
    "                    l_dbscan_cluster.insert(len(l_dbscan_cluster),l_dbscan_cluster[-1])\n",
    "            #\n",
    "            # add the nan rows back in to the beanup df\n",
    "            df_beanup = pd.concat([df_beanup,pd.DataFrame(my_nan_rows,index=my_nan_indices,columns=df_beanup.columns)],sort=False).sort_index()\n",
    "            #\n",
    "            # if we still have missing data, add more noise values\n",
    "            # (the logic above SHOULD plug these holes, so this SHOULD be unnecessary ... but never say never)\n",
    "            if len(l_dbscan_cluster) < len(df_beanup):\n",
    "                #\n",
    "                # number of missing data\n",
    "                i_dbscan_missing = len(df_beanup) - len(l_dbscan_cluster)\n",
    "                #\n",
    "                # append missing data\n",
    "                for i in range(0,i_dbscan_missing):\n",
    "                    l_dbscan_cluster.insert(0,-1)\n",
    "            #\n",
    "            # (or take them away)\n",
    "            elif len(df_beanup) < len(l_dbscan_cluster):\n",
    "                l_dbscan_cluster = l_dbscan_cluster[0:len(df_beanup)]\n",
    "            #\n",
    "            # append the cluster labels to the beanup df\n",
    "            try:\n",
    "                df_beanup.loc[:,'ii_cluster_labels'] = l_dbscan_cluster\n",
    "            except:\n",
    "                print('error at: ')\n",
    "                print(well,beanup)\n",
    "                print(len(df_beanup),len(l_dbscan_cluster))\n",
    "                df_beanup.loc[:,'ii_cluster_labels'] = l_dbscan_cluster\n",
    "            #\n",
    "            # finally, go through the clusters and merge clusters who have a lower II in the next cluster (if requested)\n",
    "            if i_want_increasing_II == 1:\n",
    "                #\n",
    "                # set up loop variables\n",
    "                my_ave_II = 0.0\n",
    "                my_prev_cluster = df_beanup['ii_cluster_labels'].iloc[0]\n",
    "                l_new_clusters = []\n",
    "                #\n",
    "                # loop over each cluster, determine average II and merge clusters if average II is less than previous one\n",
    "                for cluster in df_beanup['ii_cluster_labels'].unique():\n",
    "                    #\n",
    "                    f_ave_cluster_II = np.nanmean(df_beanup[df_beanup['ii_cluster_labels'] == cluster]['ii_gradient'])\n",
    "                    #\n",
    "                    if f_ave_cluster_II < my_ave_II:\n",
    "                        # if average II is less than before, use previous cluster number\n",
    "                        l_new_clusters += [my_prev_cluster]*len(df_beanup[df_beanup['ii_cluster_labels'] == cluster])\n",
    "                    else:\n",
    "                        my_prev_cluster = cluster   # update previous cluster to this one\n",
    "                        my_ave_II = f_ave_cluster_II  # update threshold II to merge cluster\n",
    "                        l_new_clusters += [cluster]*len(df_beanup[df_beanup['ii_cluster_labels'] == cluster])\n",
    "                    #\n",
    "                #\n",
    "                # finally, re-write cluster labels with new ones\n",
    "                try:\n",
    "                    df_beanup.loc[:,'ii_cluster_labels'] = l_new_clusters\n",
    "                except:\n",
    "                    print(len(df_beanup['ii_cluster_labels']),len(l_new_clusters))\n",
    "                    df_beanup.loc[:,'ii_cluster_labels'] = l_new_clusters\n",
    "            #\n",
    "            #\n",
    "            #########################################################################\n",
    "            # (3) injectivity measure 1 - averaged II numbers over each cluster     #\n",
    "            # (4) injectivity measure 2 - perform linear regression on each cluster #\n",
    "            #########################################################################\n",
    "            #\n",
    "            # set up an empty list to build data for injectivity df\n",
    "            l_dfinj_rows = []\n",
    "            #\n",
    "            # set up loop parameters\n",
    "            my_regime = 1                     # updates the picked flow regime\n",
    "            i_prev_counter = 0                # counts number of flow regimes prior to the one under consideration\n",
    "            my_prev_metric_II = 0.0           # previous metric II, averaged value (to compute frac pressure)\n",
    "            my_prev_metric_int = 0.0          # previous metric intercept, averaged value (to compute frac pressure)\n",
    "            my_prev_metric_reg_II = 0.0       # previous metric II, linearly regressed value (to compute frac pressure)\n",
    "            my_prev_metric_reg_int = 0.0      # previous metric intercept, linearly regressed value (to compute frac pressure)\n",
    "            #\n",
    "            # compute linear regressed and averaged injectivities for each cluster (~ flow regime)\n",
    "            for i_cluster,cluster in enumerate(df_beanup['ii_cluster_labels'].unique()):\n",
    "                #\n",
    "                # grab df for just this II cluster\n",
    "                df_cluster = df_beanup[df_beanup['ii_cluster_labels']==cluster]\n",
    "                #\n",
    "                # if this is not the first cluster, also add the previous row of pressure/rate data back into the cluster df\n",
    "                # (as we clustered II grouping on backwards-differenced derivatives, so previous value was used for this calc!)\n",
    "                if (i_cluster > 0) and ((len(df_cluster) < i_add_previous_row) or (i_add_previous_row == 0)):\n",
    "                    #\n",
    "                    # index to pull is i-1 in the original beanup df\n",
    "                    i_pull_index = (df_cluster.index[0]) - 1\n",
    "                    #\n",
    "                    if i_pull_index > 0:\n",
    "                        df_cluster = pd.concat([df_cluster,pd.DataFrame([df_beanup.iloc[df_cluster.index[0]-1].values],columns=df_cluster.columns)],sort=False).sort_index()\n",
    "                #\n",
    "                # reset the df_cluster indices for subsequent integer slicing\n",
    "                df_cluster.reset_index(drop=True,inplace=True)\n",
    "                #\n",
    "                if cluster >= 0: # don't write regime for noise (DBSCAN labels noise as -1)\n",
    "                    # \n",
    "                    # general df options\n",
    "                    flow_regime = str('regime_'+str(my_regime))   # label all regimes as regime_1, regime_2 etc.\n",
    "                    II_ave = -1.0                                 # set up token averaged II value to begin\n",
    "                    #\n",
    "                    # compute the averaged injectivity (np.nanmean() will ignore NaNs)\n",
    "                    f_ave_field_II = np.nanmean(df_cluster['ii_gradient'])\n",
    "                    f_ave_metric_II = f_ave_field_II/f_II_metric_to_field\n",
    "                    #\n",
    "                    # compute intercept from averaged injectivity (from last point in cluster) - remember true gradient = 1/II\n",
    "                    f_ave_metric_intercept = (df_cluster['pressure'].iloc[-1])-((1.0/f_ave_metric_II)*df_cluster['rate'].iloc[-1])\n",
    "                    f_ave_field_intercept = f_ave_metric_intercept*f_bar_to_psi\n",
    "                    #\n",
    "                    # set up 1D rate/pressure arrays for linear regression (have to reshape the arrays to be 1D column vectors)\n",
    "                    rate_data = np.array(df_cluster['rate'])\n",
    "                    rate_data = np.reshape(rate_data,(len(rate_data),1))\n",
    "                    pressure_data = np.array(df_cluster['pressure'])\n",
    "                    pressure_data = np.reshape(pressure_data,(len(pressure_data),1))\n",
    "                    #\n",
    "                    # perform linear regression on P(Q) points and save linear regression coefficients\n",
    "                    try:\n",
    "                        #\n",
    "                        lin_reg = LinearRegression().fit(rate_data,pressure_data)\n",
    "                        # \n",
    "                        f_lin_reg_metric_II = 1.0 / lin_reg.coef_[0][0]\n",
    "                        f_lin_reg_field_II = f_lin_reg_metric_II*f_II_metric_to_field\n",
    "                        f_lin_reg_metric_intercept = lin_reg.intercept_[0]\n",
    "                        f_lin_reg_field_intercept = f_lin_reg_metric_intercept*f_bar_to_psi\n",
    "                    #\n",
    "                    # if linear regression fails (e.g. only one point), write token noise values\n",
    "                    except:\n",
    "                        f_lin_reg_metric_II = -1.0\n",
    "                        f_lin_reg_field_II = -1.0\n",
    "                        f_lin_reg_metric_intercept = -1.0\n",
    "                        f_lin_reg_field_intercept = -1.0\n",
    "                    #\n",
    "                    # compute frac pressure / breakover (only if there is a previous cluster)\n",
    "                    # then append the data to the l_dfinj_rows list (to be later appended to df_injectivity)\n",
    "                    if i_prev_counter > 0:\n",
    "                        #\n",
    "                        # averaged values over each cluster\n",
    "                        q_breakover_ave_metric = (f_ave_metric_intercept-my_prev_metric_int)/((1.0/my_prev_metric_II)-(1.0/f_ave_metric_II))\n",
    "                        q_breakover_ave_field = q_breakover_ave_metric*f_m3hr_to_bpd\n",
    "                        p_breakover_ave_metric = f_ave_metric_intercept + (1.0/f_ave_metric_II)*q_breakover_ave_metric\n",
    "                        p_breakover_ave_field = p_breakover_ave_metric*f_bar_to_psi\n",
    "                        #\n",
    "                        # linear regression values over each cluster\n",
    "                        q_breakover_reg_metric = (f_lin_reg_metric_intercept-my_prev_metric_reg_int)/((1.0/my_prev_metric_reg_II)-(1.0/f_lin_reg_metric_II))\n",
    "                        q_breakover_reg_field = q_breakover_reg_metric*f_m3hr_to_bpd\n",
    "                        p_breakover_reg_metric = f_lin_reg_metric_intercept + (1.0/f_lin_reg_metric_II)*q_breakover_reg_metric\n",
    "                        p_breakover_reg_field = p_breakover_reg_metric*f_bar_to_psi\n",
    "                        #\n",
    "                        # store average values into list-of-lists\n",
    "                        l_dfinj_rows.append([well,\n",
    "                                            beanup,\n",
    "                                            'cluster_averaged_values',\n",
    "                                            flow_regime,\n",
    "                                            f_ave_field_II,\n",
    "                                            f_ave_metric_II,\n",
    "                                            f_ave_field_intercept,\n",
    "                                            f_ave_metric_intercept,\n",
    "                                            p_breakover_ave_field,\n",
    "                                            p_breakover_ave_metric,\n",
    "                                            q_breakover_ave_field,\n",
    "                                            q_breakover_ave_metric])\n",
    "                        #\n",
    "                        # store regressed values into list-of-lists\n",
    "                        l_dfinj_rows.append([well,\n",
    "                                            beanup,\n",
    "                                            'cluster_lin_regressed_values',\n",
    "                                            flow_regime,\n",
    "                                            f_lin_reg_field_II,\n",
    "                                            f_lin_reg_metric_II,\n",
    "                                            f_lin_reg_field_intercept,\n",
    "                                            f_lin_reg_metric_intercept,\n",
    "                                            p_breakover_reg_field,\n",
    "                                            p_breakover_reg_metric,\n",
    "                                            q_breakover_reg_field,\n",
    "                                            q_breakover_reg_metric])\n",
    "                        #\n",
    "                    #\n",
    "                    # if first regime, write average values with some token breakover values (my convention is negative = null data, rather than np.nan)\n",
    "                    else:\n",
    "                        l_dfinj_rows.append([well,\n",
    "                                            beanup,\n",
    "                                            'cluster_averaged_values',\n",
    "                                            flow_regime,\n",
    "                                            f_ave_field_II,\n",
    "                                            f_ave_metric_II,\n",
    "                                            f_ave_field_intercept,\n",
    "                                            f_ave_metric_intercept,\n",
    "                                            -1.0,\n",
    "                                            -1.0,\n",
    "                                            -1.0,\n",
    "                                            -1.0])\n",
    "                        #\n",
    "                        # store regressed values\n",
    "                        l_dfinj_rows.append([well,\n",
    "                                            beanup,\n",
    "                                            'cluster_lin_regressed_values',\n",
    "                                            flow_regime,\n",
    "                                            f_lin_reg_field_II,\n",
    "                                            f_lin_reg_metric_II,\n",
    "                                            f_lin_reg_field_intercept,\n",
    "                                            f_lin_reg_metric_intercept,\n",
    "                                            -1.0,\n",
    "                                            -1.0,\n",
    "                                            -1.0,\n",
    "                                            -1.0])\n",
    "                    #\n",
    "                    # march forward regime\n",
    "                    my_regime += 1\n",
    "                    #\n",
    "                    # march cluster counter (could actually just combine this with above, but why break an otherwise working code)\n",
    "                    i_prev_counter += 1\n",
    "                    #\n",
    "                    # save regression outputs of this cluster in case there is another regime\n",
    "                    my_prev_metric_II = f_ave_metric_II\n",
    "                    my_prev_metric_int = f_ave_metric_intercept\n",
    "                    my_prev_metric_reg_II = f_lin_reg_metric_II\n",
    "                    my_prev_metric_reg_int = f_lin_reg_metric_intercept\n",
    "                    #\n",
    "                # end of check cluster for noise\n",
    "            # end of loop over cluster for linear/averaged II\n",
    "            #\n",
    "            ####################################################################\n",
    "            # (5) injectivity measure 3 - a priori piecewise linear regression #\n",
    "            ####################################################################\n",
    "            #\n",
    "            # (this is the serious stuff ... ish)\n",
    "            #\n",
    "            # firstly, filter out noise so we are just doing piecewise regression on identified regimes\n",
    "            # (note that this can result in harsh filtering of single-point regimes; we can get around this by setting \n",
    "            #    the i_d_well_dbscan_II_minsamples values in the well dictionary to 1)\n",
    "            df_pwlf_temp = df_beanup[df_beanup['ii_cluster_labels']>=0]\n",
    "            #\n",
    "            # if requested, go through the df and add the last point from each cluster back into a new cluster\n",
    "            # (i.e. we did a backwards difference, so double up on the knot points for PWLF)\n",
    "            if i_double_knots == 1:\n",
    "                #\n",
    "                my_ii_cluster = df_pwlf_temp['ii_cluster_labels'].iloc[0]\n",
    "                my_old_row = df_pwlf_temp.iloc[0].values.tolist()\n",
    "                l_new_pwlf_data = []\n",
    "                #\n",
    "                for i_df,row_df in df_pwlf_temp.iterrows():\n",
    "                    #\n",
    "                    # if new cluster, re-append the old (last) row as an extra row (change ii_cluster_labels to new cluster)\n",
    "                    if row_df['ii_cluster_labels'] != my_ii_cluster:\n",
    "                        my_old_row[len(my_old_row)-1] = row_df['ii_cluster_labels']        \n",
    "                        l_new_pwlf_data.append(my_old_row)\n",
    "                    #\n",
    "                    # add the current row\n",
    "                    l_new_pwlf_data.append(row_df.values.tolist())\n",
    "                    #\n",
    "                    my_old_row = row_df.values.tolist()\n",
    "                    my_ii_cluster = row_df['ii_cluster_labels']\n",
    "                #\n",
    "                # re-write the df\n",
    "                df_pwlf = pd.DataFrame(l_new_pwlf_data,columns=df_pwlf_temp.columns)\n",
    "            else:\n",
    "                # otherwise just use current pulled df\n",
    "                df_pwlf = df_pwlf_temp\n",
    "            #\n",
    "            # get number of linear/knotted sections (this will feed piecewise algorithm used, otherwise we may over-fit)\n",
    "            # (i.e. the best fit piecewise linear function will have N-1 sections for N data points ... d'oh)\n",
    "            i_pwlf_sections = len(df_pwlf['ii_cluster_labels'].unique())\n",
    "            #\n",
    "            # (could add something here that if len(df_pwlf) = 3 and i_pwlf_sections = 2 then duplicate the middle point as a \n",
    "            #    human would effectively do to fit two straight lines; for now use the i_double_knots global integer flag instead)\n",
    "            #\n",
    "            # if we don't have enough points (need npoints >= 2*nsections to constrain), try reducing the number of PWLF sections;\n",
    "            i_pop = 0\n",
    "            if (len(df_pwlf) < 2*i_pwlf_sections) and (i_pwlf_sections > 1): \n",
    "                i_pwlf_sections -= 1\n",
    "                i_pop += 1\n",
    "                # double this up for added fudging in case we have e.g. 3 points, 3 II clusters / sections\n",
    "                if (len(df_pwlf) < 2*i_pwlf_sections) and (i_pwlf_sections > 1): \n",
    "                    i_pwlf_sections -= 1       \n",
    "                    i_pop += 1\n",
    "            #        \n",
    "            # build the list of guessed breakover points if we have more than one linear section \n",
    "            # (we'll use this to initialise the piecewise solver rather than give it generic guesses)\n",
    "            if i_pwlf_sections > 1:\n",
    "                #\n",
    "                # initialise list of breakover rate points\n",
    "                l_breakover_q = []\n",
    "                # \n",
    "                # grab first cluster details\n",
    "                l_cluster_label = [df_pwlf['ii_cluster_labels'].iloc[0]]\n",
    "                #\n",
    "                # initialise previous rate (used to derive mid point where the cluster changes)\n",
    "                f_prev_rate = df_pwlf['rate'].iloc[0]\n",
    "                #\n",
    "                # loop over rows and find point where new cluster begins; where it begins, record the mid-point rate (likely piecewise function breakover point)\n",
    "                i_breakover_q_points = 0 # ensure we have the right number of initial guesses with a counter, in case we reduced i_pwlf_sections above\n",
    "                for i,row in df_pwlf.iterrows():\n",
    "                    #\n",
    "                    # if the cluster in the current row is new, assign mid-point guess to the breakover rate list\n",
    "                    if (row['ii_cluster_labels'] not in l_cluster_label) and (i_breakover_q_points < i_pwlf_sections - 1):\n",
    "                        #\n",
    "                        # if new cluster row, append average rate over the cluster change\n",
    "                        l_breakover_q.append(0.5*(row['rate']+f_prev_rate))\n",
    "                        # store cluster label so we don't record this one again\n",
    "                        l_cluster_label.append(row['ii_cluster_labels'])\n",
    "                        # increment the breakpoint counter\n",
    "                        i_breakover_q_points += 1\n",
    "                        #\n",
    "                    #\n",
    "                    # update previous rate to store value from this row (so it's always \"live\")\n",
    "                    f_prev_rate = row['rate']\n",
    "                #\n",
    "                # check number of breakover points are correct (could also modify this to clip or add as required ...)\n",
    "                if len(l_breakover_q) != i_pwlf_sections - 1:\n",
    "                    print('Warning: incorrect number of initial rate guesses for piecewise linear regression on cluster data')\n",
    "                    print('Well = ',well,' Beanup = ',beanup)\n",
    "                    print('Number of linear sections: ',i_pwlf_sections - 1)\n",
    "                    print('Breakover rate guesses: ',len(l_breakover_q))\n",
    "                #\n",
    "                # finally, loop over clusters and estimate injectivity from average values\n",
    "                # (should map correctly to breakover points above ...!)\n",
    "                l_cluster_II = []\n",
    "                i_breakover_II_points = 0 # ensure we have the right number of initial guesses with a counter, in case we reduced i_pwlf_sections above\n",
    "                for cluster in df_pwlf['ii_cluster_labels'].unique():\n",
    "                    if (cluster >= 0) and (i_breakover_II_points < i_pwlf_sections):\n",
    "                        l_cluster_II.append(np.nanmean(df_pwlf[df_pwlf['ii_cluster_labels']==cluster]['ii_gradient']))\n",
    "                        i_breakover_II_points += 1\n",
    "                #\n",
    "                # similarly, check number of II guesses\n",
    "                if len(l_cluster_II) != i_pwlf_sections:\n",
    "                    print('Warning: incorrect number of initial injectivity index guesses for piecewise linear regression on cluster data')\n",
    "                    print('Well = ',well,' Beanup = ',beanup)\n",
    "                    print('Breakover sections: ',i_pwlf_sections - 1)\n",
    "                    print('II guesses: ',len(l_cluster_II))\n",
    "                #\n",
    "            #\n",
    "            # now, we'll set up piecewise linear function (can currently cope with 2, 3 or 4; if >4 then I'm assuming this isn't right)\n",
    "            # -> this ain't pretty but I'm not sure how to parametise this within np/sp\n",
    "            #\n",
    "            # also set up the initial estimates for a function with N linear parts:\n",
    "            # x0, x1, x2 are piecewise knots (breakover points, x < x0, x0 <= x < x1, ... - have N-1 values)\n",
    "            # b is the initial intercept (you can derive other intercepts from this, only have 1 value)\n",
    "            # k1, k2, k3 are gradients of each section (have N values, I've confusingly started from 1 rather than 0 to annoy and I'm too frightened to break code to change)\n",
    "            #\n",
    "            # only set up the piecewise function if we have more than one linear section \n",
    "            # (otherwise we just do a simple linear regression and there's no difference to above)\n",
    "            if i_pwlf_sections == 2:\n",
    "                #\n",
    "                # piecewise linear function definition\n",
    "                def f_pwlf(x,x0,y0,k1,k2):\n",
    "                    return np.piecewise(x, [x < x0, x >= x0], [lambda x:k1*x + y0, lambda x:k2*x + (k1-k2)*x0 + y0])\n",
    "                #\n",
    "                # guessed parameters\n",
    "                p_x0 = l_breakover_q[0]                # breakover 1 rate\n",
    "                p_y0  = df_beanup['pressure'].iloc[0]  # pressure at zero/first rate (don't ignore noise data here, use beanup df)\n",
    "                p_k1 = f_II_metric_to_field / l_cluster_II[0]   # gradient of plot, bar/m3/hr\n",
    "                p_k2 = f_II_metric_to_field / l_cluster_II[1]   #   (remember this is 1/II, also need to convert back to metric!)\n",
    "                #\n",
    "                # build guessed arguments list\n",
    "                p0 = [p_x0,p_y0,p_k1,p_k2]\n",
    "                #\n",
    "            elif i_pwlf_sections == 3:\n",
    "                #\n",
    "                # piecewise linear function definition\n",
    "                def f_pwlf(x,x0,x1,y0,k1,k2,k3):\n",
    "                    return np.piecewise(x, [x < x0, (x >= x0)&(x < x1), x >= x1], [lambda x:k1*x + y0, lambda x:k2*x + (k1-k2)*x0 + y0, lambda x:k3*x + (k2-k3)*x1 + (k1-k2)*x0 + y0])\n",
    "                #\n",
    "                # guessed parameters\n",
    "                p_x0 = l_breakover_q[0]                # breakover 1 rate\n",
    "                p_x1 = l_breakover_q[1]                # breakover 2 rate\n",
    "                p_y0  = df_beanup['pressure'].iloc[0]  # pressure at zero/first rate (don't ignore noise data here, use beanup df)\n",
    "                p_k1 = f_II_metric_to_field / l_cluster_II[0]   # gradient of plot, bar/m3/hr\n",
    "                p_k2 = f_II_metric_to_field / l_cluster_II[1]   #   (remember this is 1/II, also need to convert back to metric!)\n",
    "                p_k3 = f_II_metric_to_field / l_cluster_II[2]   #   \n",
    "                #\n",
    "                # build guessed arguments list\n",
    "                p0 = [p_x0,p_x1,p_y0,p_k1,p_k2,p_k3]\n",
    "                #\n",
    "            elif i_pwlf_sections > 3:\n",
    "                #\n",
    "                # piecewise linear function definition\n",
    "                def f_pwlf(x,x0,x1,x2,y0,k1,k2,k3,k4):\n",
    "                    return np.piecewise(x, [x < x0, (x >= x0)&(x < x1), x >= x1], [lambda x:k1*x + y0, lambda x:k2*x + (k1-k2)*x0 + y0, lambda x:k3*x + (k2-k3)*x1 + (k1-k2)*x0 + y0, lambda x:k4*x + (k3-k4)*x2 + (k2-k3)*x1 + (k1-k2)*x0 + y0])\n",
    "                #\n",
    "                # guessed parameters\n",
    "                p_x0 = l_breakover_q[0]                # breakover 1 rate\n",
    "                p_x1 = l_breakover_q[1]                # breakover 2 rate\n",
    "                p_x2 = l_breakover_q[2]                # breakover 3 rate\n",
    "                p_y0  = df_beanup['pressure'].iloc[0]  # pressure at zero rate (don't ignore noise data here, use beanup df)\n",
    "                p_k1 = f_II_metric_to_field / l_cluster_II[0]   # gradient of plot, bar/m3/hr\n",
    "                p_k2 = f_II_metric_to_field / l_cluster_II[1]   #   (remember this is 1/II, also need to convert back to metric!)\n",
    "                p_k3 = f_II_metric_to_field / l_cluster_II[2]   #   \n",
    "                p_k4 = f_II_metric_to_field / l_cluster_II[3]   #   \n",
    "                #\n",
    "                # build guessed arguments list\n",
    "                p0 = [p_x0,p_x1,p_x2,p_y0,p_k1,p_k2,p_k3,p_k4]\n",
    "                #\n",
    "            #\n",
    "            # get the rate/pressure data for the piecewise regression (don't need reshaped arrays for this as scipy is smart)\n",
    "            rate_data = np.array(df_pwlf['rate'])\n",
    "            pressure_data = np.array(df_pwlf['pressure'])\n",
    "            #\n",
    "            # perform (piecewise) linear regression and store the II data\n",
    "            #\n",
    "            if i_pwlf_sections == 1: # one section -> just do normal linear regression (get the same answer as above)\n",
    "                #\n",
    "                # this time we have to re-shape rate/pressure data for linear regression (needs to be 1D np array)\n",
    "                rate_data = np.reshape(rate_data,(len(rate_data),1))\n",
    "                pressure_data = np.reshape(pressure_data,(len(pressure_data),1))\n",
    "                #    \n",
    "                try:\n",
    "                    #\n",
    "                    lin_reg = LinearRegression().fit(rate_data,pressure_data)\n",
    "                    #\n",
    "                    # determine II and intercept from the coefficients column vector\n",
    "                    f_II_metric = 1.0 / lin_reg.coef_[0][0]\n",
    "                    f_II_field = f_II_metric * f_II_metric_to_field\n",
    "                    #\n",
    "                    f_int_metric = lin_reg.intercept_[0]\n",
    "                    f_int_field = f_int_metric * f_bar_to_psi\n",
    "                #\n",
    "                except:\n",
    "                    # if regression fails, use token values\n",
    "                    f_II_metric = -1\n",
    "                    f_II_field = -1\n",
    "                    #\n",
    "                    f_int_metric = -1\n",
    "                    f_int_field = -1\n",
    "                #\n",
    "                # store injectivity values into list to add to injectivity df\n",
    "                l_dfinj_rows.append([\n",
    "                    well,\n",
    "                    beanup,\n",
    "                    'pwlf_interp_values',\n",
    "                    'regime_1',\n",
    "                    f_II_field,\n",
    "                    f_II_metric,\n",
    "                    f_int_field,\n",
    "                    f_int_metric,\n",
    "                    -1.0,\n",
    "                    -1.0,\n",
    "                    -1.0,\n",
    "                    -1.0\n",
    "                ])\n",
    "                #\n",
    "            elif i_pwlf_sections > 1:\n",
    "                #\n",
    "                # run scipy optimiser to take initial guesses and update coefficients (in \"p\")\n",
    "                #\n",
    "                # note this will fail if number of data points < number of unknowns (which happens regularly for GL chokes!), so we'll wrap this around a try-except logic\n",
    "                # (I added some logic above to duplicate points and/or shrink number of piecewise sections to try to avoid this outcome)\n",
    "                try:\n",
    "                    #\n",
    "                    # p = list of the optimised parameters, e = estimated covariance of the optimised parameters\n",
    "                    #\n",
    "                    # we'll use default LS optimisation (Levenberg-Marquardt), fall back to more robust method if it fails\n",
    "                    p,e = optimize.curve_fit(f_pwlf,rate_data,pressure_data,p0)\n",
    "                    #\n",
    "                    # store injectivity values into list to add to injectivity df\n",
    "                    # -> for more than one regime, we also have breakovers to determine\n",
    "                    # -> also remember intercepts are not explicitly stored in optimize's p list! we need to back extract these\n",
    "                    #\n",
    "                    # first, let's pull gradient and intercept lists from the p (regressed) list\n",
    "                    l_xn = []\n",
    "                    l_kn = []\n",
    "                    #\n",
    "                    # reminder: list of parameters is [x0,x1,...],y0,[k1,k2,...]\n",
    "                    for i in range(0,i_pwlf_sections):\n",
    "                        #\n",
    "                        # get the x intercepts (remember 1 fewer than total sections)\n",
    "                        if i < (i_pwlf_sections-1) :\n",
    "                            l_xn.append(p[i])\n",
    "                        #\n",
    "                        # get the k/gradient values (equal number to total sections, located in array after the xn/y0 values)\n",
    "                        l_kn.append(p[i_pwlf_sections+i])\n",
    "                        #\n",
    "                    # get the y0 value (always a single value, remember Python list indexing starts at 0)\n",
    "                    f_y0 = p[i_pwlf_sections-1]\n",
    "                    #\n",
    "                    # back out the intercepts for each section\n",
    "                    l_intercepts = [f_y0] # first linear section is explicitly defined as y0\n",
    "                    #\n",
    "                    # other intercepts are defined one on from the other, so have to back-calculate them in each piecewise linear section\n",
    "                    for i in range(1,i_pwlf_sections):\n",
    "                        l_intercepts.append(l_intercepts[i-1] + ((l_kn[i-1]-l_kn[i])*l_xn[i-1]))\n",
    "                    #\n",
    "                    # now loop over each linear section (flow regime) and add data to the injectivity list for the df build\n",
    "                    for i in range(0,i_pwlf_sections):\n",
    "                        #\n",
    "                        # generate the regime name\n",
    "                        flow_regime = str('regime_'+str(i+1))\n",
    "                        #\n",
    "                        # get the injectivity index\n",
    "                        f_metric_II = 1.0 / l_kn[i]\n",
    "                        f_field_II = f_metric_II * f_II_metric_to_field\n",
    "                        #\n",
    "                        # get the intercept\n",
    "                        f_metric_int = l_intercepts[i]\n",
    "                        f_field_int = f_metric_int * f_bar_to_psi\n",
    "                        #\n",
    "                        # get the crossover pressure (note x-over rate is already pulled from the l_xn list)\n",
    "                        if i == 0:\n",
    "                            #\n",
    "                            # use token -1 values for first regime\n",
    "                            q_breakover_metric = -1.0\n",
    "                            q_breakover_field = -1.0\n",
    "                            p_breakover_metric = -1.0\n",
    "                            p_breakover_field = -1.0\n",
    "                            #\n",
    "                            # update the previous values\n",
    "                            f_breakover_int_prev = l_intercepts[i]\n",
    "                            f_breakover_grad_prev = l_kn[i]\n",
    "                            #\n",
    "                        else:\n",
    "                            #\n",
    "                            # generate the q/p breakover points\n",
    "                            q_breakover_metric = l_xn[i-1]\n",
    "                            p_breakover_metric = l_intercepts[i] + (l_kn[i]*q_breakover_metric)\n",
    "                            #\n",
    "                            # convert to field units\n",
    "                            q_breakover_field = q_breakover_metric * f_m3hr_to_bpd\n",
    "                            p_breakover_field = p_breakover_metric * f_bar_to_psi\n",
    "                            #\n",
    "                        #\n",
    "                        # append data to injectivity list build\n",
    "                        l_dfinj_rows.append([\n",
    "                            well,\n",
    "                            beanup,\n",
    "                            'pwlf_interp_values',\n",
    "                            flow_regime,\n",
    "                            f_field_II,\n",
    "                            f_metric_II,\n",
    "                            f_field_int,\n",
    "                            f_metric_int,\n",
    "                            p_breakover_field,\n",
    "                            p_breakover_metric,\n",
    "                            q_breakover_field,\n",
    "                            q_breakover_metric\n",
    "                        ])\n",
    "                        #\n",
    "                except:\n",
    "                    pass # if we have a function fail, pass over this beanup\n",
    "                # (end of loop over piecewise linear sections)\n",
    "            # (end of check number of piecewise linear sections)\n",
    "            #\n",
    "            # finally, add all the collated data from this beanup to the injectivity df\n",
    "            df_injectivity = pd.concat([df_injectivity,pd.DataFrame(l_dfinj_rows,columns=df_injectivity.columns)],ignore_index=True)\n",
    "            #\n",
    "        # (end loop over beanups for a well)\n",
    "    # (end loop over all wells)\n",
    "    #\n",
    "    # add the initial pressures and temperatures back in to the injectivity df\n",
    "    for well in df_injectivity['well'].unique():\n",
    "        #\n",
    "        # well data\n",
    "        df_well = df[df['well']==well][['injector_period_ID','pressure_initial','stab_type','rate','temperature']]\n",
    "        #\n",
    "        # slice df_well by the requested stabilisation type\n",
    "        df_well = df_well[df_well['stab_type'] == d_well_data[well][i_d_well_stabilisation_method]]\n",
    "        #\n",
    "        # build new df of injector period -> initial pressure / temperature\n",
    "        l_beanup_p0 = []\n",
    "        for beanup in df_well['injector_period_ID'].unique():\n",
    "            #\n",
    "            # get df for this beanup only\n",
    "            df_beanup = df_well[df_well['injector_period_ID'] == beanup].reset_index(drop=True)\n",
    "            #\n",
    "            # loop over df and find (initial) pressure, temperature at min rate\n",
    "            my_min_rate = df_beanup['rate'].iloc[0]\n",
    "            my_initial_pressure = df_beanup['pressure_initial'].iloc[0]\n",
    "            my_initial_temperature = df_beanup['temperature'].iloc[0]\n",
    "            #\n",
    "            for i_df,r_df in df_beanup.iterrows():\n",
    "                if r_df['rate'] < my_min_rate:\n",
    "                    my_initial_pressure = r_df['pressure_initial']\n",
    "                    my_min_rate = r_df['rate']\n",
    "                    my_initial_temperature = r_df['temperature']\n",
    "                #\n",
    "            #\n",
    "            # store this to list\n",
    "            l_beanup_p0.append([beanup,my_initial_pressure,my_initial_temperature])\n",
    "        #\n",
    "        # append list to new df\n",
    "        df_p0 = pd.DataFrame(l_beanup_p0,columns=['injector_period_ID','pressure_initial','temperature'])\n",
    "        #\n",
    "        # merge dfs (clip the beanup II data set to the well so we don't inadvertently merge other wells)\n",
    "        df_well_injectivity = df_injectivity[df_injectivity['well']==well].merge(df_p0,how='inner',on='injector_period_ID')\n",
    "        #\n",
    "        # concatenate all wells\n",
    "        try:\n",
    "            df_injectivity_merged = pd.concat([df_injectivity_merged,df_well_injectivity],ignore_index=True)\n",
    "        except:\n",
    "            df_injectivity_merged = df_well_injectivity.copy(deep=True)\n",
    "        #\n",
    "        print(str('Processed bean-up injectivity estimations for well: '+well+' - number processed: '+str(len(df_well_injectivity['injector_period_ID'].unique()))))\n",
    "    #\n",
    "    # finally (finally), go through the bean-up df and clip outliers based on rules for frac pressure, II etc.\n",
    "    for well in df_injectivity_merged['well'].unique():\n",
    "        #\n",
    "        f_pfrac_clip = d_well_data[well][i_d_well_minfracpressure]\n",
    "        f_pfrac_max_clip = d_well_data[well][i_d_well_maxPfrac]\n",
    "        f_qfrac_clip = d_well_data[well][i_d_well_maxQfrac]\n",
    "        f_II_clip = d_well_data[well][i_d_well_maxII]\n",
    "        #\n",
    "        df_well_clip = df_injectivity_merged[df_injectivity_merged['well'] == well].copy(deep=True)\n",
    "        #\n",
    "        # hard force frac breakover columns below clip to -1, also clip frac breakover rates (on 0-200 m3/hr for the time being ...)\n",
    "        df_well_clip.loc[(df_well_clip['Qfrac_metric']<=0.0) | (df_well_clip['Qfrac_metric']>=f_qfrac_clip),['Qfrac_field','Qfrac_metric']] = -1\n",
    "        df_well_clip.loc[(df_well_clip['Pfrac_metric']<f_pfrac_clip) | (df_well_clip['Pfrac_metric']>f_pfrac_max_clip),['Pfrac_field','Pfrac_metric','Qfrac_field','Qfrac_metric']] = -1\n",
    "        #\n",
    "        # clip all II to 0/max II to NaN (need bitwise OR for series logic ...)\n",
    "        df_well_clip.loc[(df_well_clip['II_metric']<=0.0)|(df_well_clip['II_metric']>f_II_clip),['II_field','II_metric']] = np.nan\n",
    "        #\n",
    "        # drop the NaN rows to get rid of spurious beanups\n",
    "        df_well_clip.dropna(how='any',inplace=True)\n",
    "        #\n",
    "        # glue the well dfs together\n",
    "        try:\n",
    "            df_injectivity_clipped = pd.concat([df_injectivity_clipped,df_well_clip],ignore_index=True)\n",
    "        except:\n",
    "            df_injectivity_clipped = df_well_clip.copy(deep=True)\n",
    "    #\n",
    "    print(str('Clipped II outliers. DF length reduced from '+str(len(df_injectivity_merged))+' to '+str(len(df_injectivity_clipped))+'.'))\n",
    "    print(str('Completed bean-up II calculations.'))\n",
    "    #\n",
    "    # return the injectivity df\n",
    "    return df_injectivity_clipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beanup_II_data(PRES_data_all, Process_WI_data, beanup_II_tracker, merged_beanup_clusters, steady_state_II_data, beanup_QC):\n",
    "    #\n",
    "    # Transform to add extra detail to the beanup II calculated, such as water quality and far-field reservoir pressure\n",
    "    #\n",
    "    # get the data sets (make sure to sort by datetime for the interpolation in this transform)\n",
    "    # (also remane the time column to be consistent)\n",
    "    #\n",
    "    # bean-up injectivity / frac breakover data\n",
    "    df_buII = beanup_II_tracker.copy(deep=True)\n",
    "    # water quality data\n",
    "    df_wiquality = Process_WI_data.copy(deep=True).rename({'time':'datetime'},axis='columns').sort_values(by='datetime')\n",
    "    # reservoir pressure data (from RDOL tracker)\n",
    "    df_PRES = PRES_data_all.copy(deep=True).rename({'time':'datetime'},axis='columns').sort_values(by='datetime')\n",
    "    # bean-up datetimes from PQ cluster data\n",
    "    df_bu_datetimes = merged_beanup_clusters.copy(deep=True)[['datetime','well','injector_period_ID']] # we'll use this to map bean-up injector period ID to datetime\n",
    "    # steady-state II data (we'll pull the near-field PRES data from this)\n",
    "    df_ss_data = steady_state_II_data.copy(deep=True)[['well','injector_period_ID','near_field_PRES_bar','shutin_length_d']]\n",
    "    # bean-up QC data \n",
    "    df_bu_qc = beanup_QC.copy(deep=True)\n",
    "    #\n",
    "    # set up the WI quality df, don't need all the rows\n",
    "    df_wiquality = df_wiquality[['datetime','sw_blend','oiw_diluted','o2_diluted']]\n",
    "    #\n",
    "    # do the same for the PRES data\n",
    "    df_PRES = df_PRES[['Well','datetime','Datum_PRES_psi_corrected']]\n",
    "    #\n",
    "    # loop over each well and pull various bits of data together to apply metadata to the bean up\n",
    "    for well in df_buII['well'].unique():\n",
    "        #\n",
    "        # get the bean-up dates for this well\n",
    "        df_bu_dates = df_bu_datetimes[df_bu_datetimes['well']==well].drop(['well'],axis='columns')\n",
    "        #\n",
    "        # go through beanups and derive dates, lengths of each bean up\n",
    "        l_beanup_data = []\n",
    "        for beanup in df_bu_dates['injector_period_ID'].unique():\n",
    "            #\n",
    "            # slice the df to get the beanup date times only\n",
    "            df_beanup = df_bu_dates[df_bu_dates['injector_period_ID'] == beanup].sort_values(by='datetime')\n",
    "            #\n",
    "            # take first datetime as start point and take range as the length; convert to mins\n",
    "            l_beanup_data.append([beanup,df_beanup['datetime'].iloc[0],(df_beanup['datetime'].iloc[-1] - df_beanup['datetime'].iloc[0]).total_seconds() / 60.0])\n",
    "            #\n",
    "        #\n",
    "        # build new df with these data for merging into the main df\n",
    "        df_beanup_data = pd.DataFrame(l_beanup_data,columns=['injector_period_ID','datetime','beanup_length_min'])\n",
    "        #\n",
    "        # pull the bean-up data, don't drop any of the columns; sort by injector period ID should make it chronological\n",
    "        df_buII_well = df_buII[df_buII['well']==well].sort_values(by='injector_period_ID')\n",
    "        #\n",
    "        # add datetime column to the bean-up data set and sort by this, shouldnt' make a difference \n",
    "        df_buII_well = df_buII_well.merge(df_beanup_data,how='left',on='injector_period_ID').sort_values(by='datetime')\n",
    "        #\n",
    "        # add the PWRI data using global interpolation function\n",
    "        df_buII_well.loc[:,'sw_blend'] = interpolate_df_col(df_buII_well['datetime'],df_wiquality['datetime'],df_wiquality['sw_blend'],'previous')\n",
    "        df_buII_well.loc[:,'oiw_diluted'] = interpolate_df_col(df_buII_well['datetime'],df_wiquality['datetime'],df_wiquality['oiw_diluted'],'previous')\n",
    "        df_buII_well.loc[:,'o2_diluted'] = interpolate_df_col(df_buII_well['datetime'],df_wiquality['datetime'],df_wiquality['o2_diluted'],'previous')\n",
    "        #\n",
    "        # add the shut-in near well data \n",
    "        l_pbu_wells = d_well_data[well][i_d_well_offset]\n",
    "        #\n",
    "        # slice the PRES data for these wells\n",
    "        # (for now, we'll use ALL PRES values; in the future add something here for the PBU_PFO_type)\n",
    "        df_pres_well = df_PRES[df_PRES['Well'].isin(l_pbu_wells)].rename({'Datum_PRES_psi_corrected':'far_field_PRES_bar'},axis='columns').drop(['Well'],axis='columns')\n",
    "        #\n",
    "        # convert the psi to bar\n",
    "        df_pres_well.loc[:,'far_field_PRES_bar'] = df_pres_well['far_field_PRES_bar'].divide(f_bar_to_psi)\n",
    "        #\n",
    "        # add the far field PRES data to the data set (both previous fill and linear so you can choose your interpolation poison)\n",
    "        df_buII_well.loc[:,'far_field_PRES_pad_bar'] = interpolate_df_col(df_buII_well['datetime'],df_pres_well['datetime'],df_pres_well['far_field_PRES_bar'],'previous')\n",
    "        df_buII_well.loc[:,'far_field_PRES_linear_bar'] = interpolate_df_col(df_buII_well['datetime'],df_pres_well['datetime'],df_pres_well['far_field_PRES_bar'],'linear')\n",
    "        #\n",
    "        # add the QC score for the bean-up\n",
    "        df_well_qc = df_bu_qc[df_bu_qc['well']==well].drop(['well'],axis='columns').sort_values(by='injector_period_ID')\n",
    "        df_buII_well = df_buII_well.merge(df_well_qc,how='left',on='injector_period_ID').sort_values(by='datetime')\n",
    "        #\n",
    "        # finally, add in near-well reservoir pressure and shut-in length\n",
    "        df_ssII_well = df_ss_data[df_ss_data['well']==well].drop(['well'],axis='columns').sort_values(by='injector_period_ID')\n",
    "        #\n",
    "        # build list of beanup data, note st-st period must be preceded by bean-up\n",
    "        l_ss_beanup_map = []\n",
    "        for beanup in df_ssII_well['injector_period_ID'].unique():\n",
    "            #\n",
    "            # get beanup df (easier to grab values, near-field PRES / time should be same for all rows of beanup)\n",
    "            df_beanup = df_ssII_well[df_ssII_well['injector_period_ID'] == beanup]\n",
    "            #\n",
    "            # add mapping function to list\n",
    "            l_ss_beanup_map.append([beanup-1,df_beanup['near_field_PRES_bar'].iloc[0],df_beanup['shutin_length_d'].iloc[0]])\n",
    "        #\n",
    "        # generate df to merge\n",
    "        df_ss_well_data = pd.DataFrame(l_ss_beanup_map,columns=['injector_period_ID','near_field_PRES_bar','shutin_length_d'])\n",
    "        #\n",
    "        # merge near-field pres / shut-in length data with bean-up df\n",
    "        df_buII_well = df_buII_well.merge(df_ss_well_data,how='left',on='injector_period_ID').sort_values(by='datetime')\n",
    "        #\n",
    "        # append to global df\n",
    "        try:\n",
    "            df_buII_well_merge = pd.concat([df_buII_well_merge,df_buII_well],ignore_index=True)\n",
    "        except:\n",
    "            df_buII_well_merge = df_buII_well.copy(deep=True)\n",
    "        #\n",
    "        print(str('Completed beanup II data union for well '+well))\n",
    "    #\n",
    "    return df_buII_well_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stst_II_plot_wells(steady_state_II_data):\n",
    "    #\n",
    "    df = steady_state_II_data.copy(deep=True)\n",
    "    #\n",
    "    plot_wells = list(d_well_data.keys)\n",
    "    #\n",
    "    # set up scatter plot of the steady-state data\n",
    "    plt.subplots(figsize=(15,7))\n",
    "    #\n",
    "    plt_colour_list = ['blue','mediumturquoise','darkgreen','darkviolet','darkgoldenrod','grey','gold','darkturquoise','darkgoldenrod','limegreen','saddlebrown','black','deeppink','greenyellow','darkmagenta']\n",
    "    #\n",
    "    for i,well in enumerate(plot_wells):\n",
    "        #\n",
    "        # slice df for well\n",
    "        df_ststIIplot = df[df['well']==well].copy(deep=True).sort_values(by='datetime')\n",
    "        #\n",
    "        i_p = 0\n",
    "        l_stst_II = []\n",
    "        l_stst_dt = []\n",
    "        #\n",
    "        for period in df_ststIIplot['injector_period_ID'].unique():\n",
    "            df_period = df_ststIIplot[df_ststIIplot['injector_period_ID']==period].reset_index(drop=True)\n",
    "            #\n",
    "            # get the middle period (we'll use this to plot the main II for this interval)\n",
    "            i_cut = (int(np.nanmean(df_period.index.tolist())))\n",
    "            #\n",
    "            # plot single value II point for the steady-state time\n",
    "            if i_p == 0:\n",
    "                plt.scatter(df_period['datetime'].iloc[i_cut],df_period['II_field'].iloc[i_cut],c=plt_colour_list[i],label=well)\n",
    "                i_p = 1\n",
    "            else:\n",
    "                plt.scatter(df_period['datetime'].iloc[i_cut],df_period['II_field'].iloc[i_cut],c=plt_colour_list[i],label='_nolabel_')\n",
    "            #\n",
    "            # rolling average plotter - change as necessary\n",
    "            if (df_period['II_field'].iloc[i_cut] > 0.0) and (df_period['II_field'].iloc[i_cut] < 30.0):\n",
    "                l_stst_II.append(df_period['II_field'].iloc[i_cut])\n",
    "                l_stst_dt.append(df_period['datetime'].iloc[i_cut])\n",
    "            stst_rolling = pd.DataFrame()\n",
    "            stst_rolling.loc[:,'datetime'] = l_stst_dt\n",
    "            stst_rolling.loc[:,'II_field'] = l_stst_II\n",
    "            plt.plot(stst_rolling['datetime'],stst_rolling['II_field'].rolling(3,center=True).mean(),color=plt_colour_list[i],alpha=0.5,label='_nolabel_')\n",
    "            #\n",
    "            # plot ALL steady-state II with alpha blend\n",
    "            plt.plot(df_period['datetime'],df_period['II_field'],color=plt_colour_list[i],alpha=0.15,label='_nolabel_')\n",
    "        #\n",
    "    # Plot options\n",
    "    plt.ylabel('Steady-state injectivity (stb/d/psi)')\n",
    "    plt.legend()\n",
    "    # plt.ylim([0,100])\n",
    "    plt.ylim([0,30])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_beanup_II(beanup_II_data):\n",
    "    #\n",
    "    # well to plot\n",
    "    plot_well = 'AW11'\n",
    "    #\n",
    "    df = beanup_II_data.copy(deep=True)\n",
    "    df_plot = df[df['well']==plot_well]\n",
    "    #\n",
    "    plt.subplots(figsize=(15,15))\n",
    "    #\n",
    "    # colours per regime\n",
    "    plot_colour = ['blue','red','green','orange']\n",
    "    #\n",
    "    # scatter points per measure\n",
    "    plot_marker = ['|','o','x']\n",
    "    #\n",
    "    # bean-up II\n",
    "    plt.subplot(311)\n",
    "    for i_colour,flow_regime in enumerate(df_plot['flow_regime'].unique()):\n",
    "        df_flow = df_plot[df_plot['flow_regime'] == flow_regime]\n",
    "        for i_marker,II_measure in enumerate(df_flow['II_measure'].unique()):\n",
    "            # ignore -1 values\n",
    "            df_measure = df_flow[df_flow['II_measure'] == II_measure]\n",
    "            plt.scatter(df_measure['injector_period_ID'],df_measure['II_field'],label=str(str(flow_regime)+' '+str(II_measure)),color=plot_colour[i_colour],marker=plot_marker[i_marker])\n",
    "    #\n",
    "    plt.legend()\n",
    "    plt.xlabel('Injection period ID')\n",
    "    plt.ylabel('Measured II (stb/d/psi)')\n",
    "    plt.ylim(0,100)\n",
    "    #\n",
    "    # frac pressure\n",
    "    plt.subplot(312)\n",
    "    for i_colour,flow_regime in enumerate(df_plot['flow_regime'].unique()):\n",
    "        df_flow = df_plot[df_plot['flow_regime'] == flow_regime]\n",
    "        for i_marker,II_measure in enumerate(df_flow['II_measure'].unique()):\n",
    "            df_measure = df_flow[df_flow['II_measure'] == II_measure]\n",
    "            # ignore -1 values\n",
    "            df_measure = df_measure[df_measure['Pfrac_metric']>0]\n",
    "            plt.scatter(df_measure['injector_period_ID'],df_measure['Pfrac_metric'],label=str(str(flow_regime)+' '+str(II_measure)),color=plot_colour[i_colour],marker=plot_marker[i_marker])\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.xlabel('Injection period ID')\n",
    "    plt.ylabel('Measured frac breakover (barg)')\n",
    "    #\n",
    "    # frac rate\n",
    "    plt.subplot(313)\n",
    "    for i_colour,flow_regime in enumerate(df_plot['flow_regime'].unique()):\n",
    "        df_flow = df_plot[df_plot['flow_regime'] == flow_regime]\n",
    "        for i_marker,II_measure in enumerate(df_flow['II_measure'].unique()):\n",
    "            df_measure = df_flow[df_flow['II_measure'] == II_measure]\n",
    "            # ignore -1 values\n",
    "            df_measure = df_measure[df_measure['Qfrac_metric']>0]\n",
    "            plt.scatter(df_measure['injector_period_ID'],df_measure['Qfrac_metric'],label=str(str(flow_regime)+' '+str(II_measure)),color=plot_colour[i_colour],marker=plot_marker[i_marker])\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.xlabel('Injection period ID')\n",
    "    plt.ylabel('Measured frac breakover rate (m3/hr)')    \n",
    "    #plt.ylim(100,400)\n",
    "    #\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beanup_Pfrac_distribution(beanup_II_data):\n",
    "    #\n",
    "    plot_well = list(d_well_data.keys)\n",
    "    #\n",
    "    df_plot = beanup_II_data.copy(deep=True)\n",
    "    df_plot = df_plot[(df_plot['well']==plot_well)&(df_plot['Pfrac_metric']>=d_well_data[plot_well][i_d_well_minfracpressure])]\n",
    "    #\n",
    "    fig,ax = plt.subplots(figsize=(15,5))\n",
    "    #\n",
    "    # plot the data histogram pdf\n",
    "    ax.hist(df_plot['Pfrac_metric'],bins=20,density=True)\n",
    "    plt.xlabel('Observed fracture breakover pressure (barg)')\n",
    "    ax.set_ylabel('PDF')\n",
    "    #\n",
    "    # plot the data histogram cdf\n",
    "    ax1 = ax.twinx()\n",
    "    ax1.hist(df_plot['Pfrac_metric'],bins=20,density=True,cumulative=True,histtype='step',color='black')\n",
    "    ax1.set_ylabel('CDF')\n",
    "    #\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
